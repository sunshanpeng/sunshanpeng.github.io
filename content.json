{"meta":{"title":"阿鹏的技术博客","subtitle":null,"description":"技术博客","author":"sunshanpeng","url":"http://yoursite.com","root":"/"},"pages":[],"posts":[{"title":"Kubernetes介绍","slug":"Kubernetes介绍","date":"2019-02-24T14:52:37.000Z","updated":"2019-02-24T14:52:55.478Z","comments":true,"path":"2019/02/24/Kubernetes介绍/","link":"","permalink":"http://yoursite.com/2019/02/24/Kubernetes介绍/","excerpt":"","text":"有了Docker之后，单个应用进程的使用管理变的更简单了，但是如果想要管理一个集群的不同应用，处理应用之间的关系， 比如：一个Web应用与数据库之间的访问关系，负载均衡和后端服务之间的代理关系，门户应用与授权组件的调用关系， 再比如说一个服务单位的不同功能之间的关系，如一个Web应用与日志搜集组件之间的文件交换关系， 在传统虚拟机环境对这种关系的处理方法都比较“粗颗粒”，一股脑的部署在同一台虚拟机中，需要手动维护很多跟应用协作的守护进程，用来处理它的日志搜集、灾难恢复、数据备份等辅助工作。 使用容器和编排技术以后，那些原来挤在同一个虚拟机里的各个应用、组件、守护进程，都可以被分别做成镜像，然后运行在一个个专属的容器中。 它们之间互不干涉，拥有各自的资源配额，可以被调度在整个集群里的任何一台机器上。 如果只是用来封装微服务、拉取用户镜像、调度单容器，用Docker Swarm就足够了，而且方便有效，但是如果需要路由网关、水平扩展、弹性伸缩、监控、备份、灾难恢复等一系列运维能力，就需要使用kubernetes来解决了。 Kubernetes介绍 Kubernetes是Google开源的容器编排调度引擎，它的目标不仅仅是一个编排系统，而是提供一个规范，可以用来描述集群的架构，定义服务的最终状态，Kubernetes可以将系统自动得到和维持在这个状态。 更直白的说，Kubernetes用户可以通过编写一个yaml或者json格式的配置文件，也可以通过工具/代码生成或直接请求Kubernetes API创建应用，该配置文件中包含了用户想要应用程序保持的状态，不论整个Kubernetes集群中的个别主机发生什么问题，都不会影响应用程序的状态，你还可以通过改变该配置文件或请求Kubernetes API来改变应用程序的状态。 配置文件里面怎么定义的，整个系统就是怎么运行的。 kubernetes 的马洛斯需求 Kubernetes架构图 Kubernets属于主从的分布式集群架构，包含Master和Node： Master作为控制节点，调度管理整个系统，包含以下组件： API Server作为kubernetes系统的入口，封装了核心对象的增删改查操作，以RESTFul接口方式提供给外部客户和内部组件调用。它维护的REST对象将持久化到etcd(一个分布式强一致性的key/value存储)。 Scheduler：负责集群的资源调度，为新建的pod分配机器。这部分工作分出来变成一个组件，意味着可以很方便地替换成其他的调度器。 Controller Manager：负责执行各种控制器，目前有两类：（1）Endpoint Controller：定期关联service和pod(关联信息由endpoint对象维护)，保证service到pod的映射总是最新的。（2）Replication Controller：定期关联replicationController和pod，保证replicationController定义的复制数量与实际运行pod的数量总是一致的。 Node是运行节点，运行业务容器，包含以下组件： Kubelet：负责管控docker容器，如启动/停止、监控运行状态等。它会定期从etcd获取分配到本机的pod，并根据pod信息启动或停止相应的容器。同时，它也会接收apiserver的HTTP请求，汇报pod的运行状态。 Kube Proxy：负责为pod提供代理。它会定期从etcd获取所有的service，并根据service信息创建代理。当某个客户pod要访问其他pod时，访问请求会经过本机proxy做转发。 Kubernets使用Etcd作为存储和通信中间件，实现Master和Node的一致性，这是目前比较常见的做法，典型的SOA架构，解耦Master和Node。 基本概念 PodPod是Kubernetes的基本操作单元，把相关的一个或多个容器构成一个Pod，通常Pod里的容器运行相同的应用。Pod包含的容器运行在同一个Node(Host)上，看作一个统一管理单元，共享相同的volumes和network namespace/IP和Port空间。 Replication ControllerReplication Controller确保任何时候Kubernetes集群中有指定数量的pod副本(replicas)在运行， 如果少于指定数量的pod副本(replicas)，Replication Controller会启动新的Container，反之会杀死多余的以保证数量不变。 Replication Controller使用预先定义的pod模板创建pods，一旦创建成功，pod 模板和创建的pods没有任何关联，可以修改pod 模板而不会对已创建pods有任何影响，也可以直接更新通过Replication Controller创建的pods。 对于利用pod 模板创建的pods，Replication Controller根据label selector来关联，通过修改pods的label可以删除对应的pods。 ServiceService也是Kubernetes的基本操作单元，是真实应用服务的抽象，每一个服务后面都有很多对应的容器来支持，通过Proxy的port和服务selector决定服务请求传递给后端提供服务的容器，对外表现为一个单一访问接口，外部不需要了解后端如何运行，这给扩展或维护后端带来很大的好处。 LabelLabel是用于区分Pod、Service、Replication Controller的key/value键值对，Pod、Service、 Replication Controller可以有多个label，但是每个label的key只能对应一个value。Labels是Service和Replication Controller运行的基础，为了将访问Service的请求转发给后端提供服务的多个容器，正是通过标识容器的labels来选择正确的容器。同样，Replication Controller也使用labels来管理通过pod 模板创建的一组容器，这样Replication Controller可以更加容易，方便地管理多个容器，无论有多少容器。 注 本文内容是在极客时间学习张磊老师《深入剖析Kubernetes》专栏之后的一点理解，如果有写的不对的地方欢迎留言指正，另外也希望大家支持张磊老师，支持极客时间，支持知识付费，多多学习，共同进步。","categories":[{"name":"容器","slug":"容器","permalink":"http://yoursite.com/categories/容器/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"http://yoursite.com/tags/k8s/"}]},{"title":"Pod的意义","slug":"Pod的意义","date":"2019-02-24T14:29:37.000Z","updated":"2019-03-07T01:27:04.778Z","comments":true,"path":"2019/02/24/Pod的意义/","link":"","permalink":"http://yoursite.com/2019/02/24/Pod的意义/","excerpt":"","text":"Kubernetes 使用 Pod 来管理容器，每个 Pod 可以包含一个或多个紧密关联的容器。 Pod 是一组紧密关联的容器集合，它们共享 PID、IPC、Network 和 UTS namespace，是 Kubernetes 调度的基本单位。 Pod 内的多个容器共享网络和文件系统，可以通过进程间通信和文件共享这种简单高效的方式组合完成服务。 Pod的意义容器都是单进程运行的，不是因为容器只能运行一个进程，而是容器没有管理多个进程的能力。 容器里PID=1的进程就是应用本身，其他的进程都是这个PID=1进程的子进程。如果启动了第二个进程，那这第二个进程异常退出的时候，外部并不能感知，也就管理不了。 在一个真正的操作系统里，进程并不是“孤苦伶仃”地独自运行的，应用之间可能有着密切的协作关系，必须部署在同一台机器上。 这些密切关系包括但不限于：互相之间会发生直接的文件交换，使用localhost或者Socket文件进行本地通信，会发生非常频繁的远程调用，共享某些Linux Namespace。 Pod的实现原理首先，Pod只是一个逻辑概念，是一组共享了某些资源的容器。 Kubernetes真正处理的，还是宿主机操作系统上Linux容器的Namespace和Cgroups，而并不存在一个所谓的Pod的边界或者隔离环境。 Kubernetes里，Pod的实现需要使用一个中间容器，叫做Infra容器，在Pod中，Infra容器永远都是第一个被创建的容器。 其他用户自定义的容器，通过Join Network Namespace的方式与Infra容器关联在一起。 如图所示，这个Pod里除了两个用户容器，还有一个Infra容器。 这个Infra容器占用极少的资源，使用了一个非常特殊的镜像：k8s.gcr.io/pause，这个镜像永远处于“暂停”状态，100-200KB左右大小。 Infra容器生成NetWork Namespace后，用户容器加入Infra容器的Network Namespace中，用户容器和Infra容器在宿主机上的Namespace文件是一样的。 这意味着，对Pod中的容器A和容器B来说： 它们可以直接使用localhost进行通信； 他们看到的网络设备和Infra容器的一样； 一个Pod只有一个IP地址，也就是容器A、容器B、Infra容器的ip地址一致； 所有的网络资源被改Pod中的所有容器共享； Pod的生命周期只跟Infra容器一致，与容器A和B无关。 对于同一个Pod里面的所有用户容器来说，它们的进出流量，都是通过Infra容器完成的。 Pod的本质，实际上是在扮演传统基础设施“虚拟机”的角色；容器则是这个虚拟机里运行的用户进程。 注本文内容是在极客时间学习张磊老师《深入剖析Kubernetes》专栏之后的一点理解，如果有写的不对的地方欢迎留言指正，另外也希望大家支持张磊老师，支持极客时间，支持知识付费，多多学习，共同进步。","categories":[{"name":"容器","slug":"容器","permalink":"http://yoursite.com/categories/容器/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"http://yoursite.com/tags/k8s/"}]},{"title":"Docker的本质","slug":"Docker的本质","date":"2019-02-24T14:26:37.000Z","updated":"2019-02-24T14:37:53.650Z","comments":true,"path":"2019/02/24/Docker的本质/","link":"","permalink":"http://yoursite.com/2019/02/24/Docker的本质/","excerpt":"","text":"Docker容器其实是一种沙盒技术，能够像一个集装箱一样，把应用“装”起来。 这样应用与应用之间，就因为有了边界而不会相互干扰； 被装进集装箱的应用，也可以被方便地搬来搬去。 容器的本质，是由Namespace、Cgroups和rootfs三种技术构建出来的进程的隔离环境。 Docke容器技术的核心功能，是通过约束和修改进程的动态表现，从而为其创造出一个“边界”。 Namespace技术是用来修改进程视图的主要方法，Cgroups技术是用来制造约束的主要手段，rootfs技术是用来限制进程文件系统的主要方式。 Namespace：正常情况下，每当我们在宿主机上运行一个程序的时候，操作系统都会给它分配一个进程编号，比如PID=100。这是进程的唯一标识，就像员工的工号一样。 而通过Docker把程序运行在容器中的时候，利用Namespace技术给进程施一个“障眼法”，让它看不到其他的进程，误以为只有它一个进程。 这种机制对被隔离的应用的进程空间做了限制，使其只能看到重新计算过的进程编号，比如PID=1，但事实上它在宿主机的操作系统里，还是原来的100号进程。 除了PID Namespace，Linux系统还提供了Mount、UTS、IPC、Network和User这些Namespace，用来对各种不同的进程上下文进行“障眼法”操作。 比如用Mount Namespace让被隔离的进程只能看到当前Namespace里的挂载点信息；Network Namespace让被隔离的进程只能看到当前Namespace的网络设备和配置。 所以，实际上Docker容器只是指定了这个进程所需要启用的一组Namespace参数，从而让进程只能看到当前Namespace所限定的资源、文件、设备、状态和配置等，对宿主机和其他不相关的程序完全看不到。 所以也可以说，容器其实就是一种特殊的进程而已。 虚拟机和Docker容器比较 在理解Docker的时候，通常会用这张图来比较虚拟机和Docker。 左边画出了虚拟机的工作原理：其中Hypervisor是虚拟机最主要的部分，它通过硬件虚拟化功能，模拟出运行一个操作系统需要的各种硬件，比如CPU、内存、I/O设备等等。然后它在这些虚拟的硬件上安装了一个新的操作系统，即Guest OS。 这样，用户的应用进程就可以运行在这个虚拟机的机器中，它能够看到的自然也只有Guest OS的文件和目录，以及这个机器里的虚拟设备。 右边用一个名为Docker Engine的软件替换了Hypervisor，把虚拟机的概念套在了容器上，实际上这个说法是不严谨的，跟真正存在的虚拟机不同，实际上并没有Docker Engine这一层。 Docker帮助用户启动的还是原来的应用进程，只不过在创建这些进程时给它们加上了各种各样的Namespace参数。 这样，这些进程就会觉得自己是各自PID Namespace里的第1号进程，只能看到各自Mount Namespace里挂载的目录和文件，只能访问各自Network Namespace里的网络设备，就好像运行在一个个独立的容器里。 Docker的架构其实这样画更合适 如果用虚拟化技术作为应用沙盒，就必须要由Hypervisor来负责创建虚拟机，这个虚拟机是真实存在的，并且它里面必须运行一个完整的Guest OS才能执行用户的应用进程。这就不可避免的带来的额外的资源消耗和占用。 根据实验，一个运行着CentOS的KVM虚拟机启动后，在不做优化的情况下，虚拟机自己就需要占用100~·200内存。此外，用户应用运行在虚拟机里面，对宿主机操作系统的调用就不可避免地进过虚拟化软件的拦截和处理，这本身又是一层性能损失。 使用Docker容器化后的用户应用，依然还是一个宿主机上的普通进程，这就意味着因为虚拟化而带来的性能损耗都是不存在的； 另一方面，使用Namespace作为隔离手段的容器并不需要单独的Guest OS，这使得容器额外的资源占用几乎可以忽略不计。 所以“敏捷”和“高性能”是容器相较于虚拟化最大的优势。 当然有利就有弊，Docker容器相较于虚拟化最大的弊端就是隔离的不彻底。 因为容器只是运行在宿主机上的一种特殊的进程，所以多个容器之间使用的还是同一个宿主机的操作系统内核。 另外，很多资源和对象不能被Namespace化，比如：时间。 Cgroups：虽然容器内的进程在Namespace的作用下只能看到容器里的情况，但是在宿主机上，它作为第100号进程，与其它所有的进程之间依然是平等的竞争关系。 这就意味着，虽然这个进程表面上被隔离起来了，但它所能够使用到的资源（CPU、内存），却是可以随时被宿主机的其他进程（或者其他容器）占用。 当然这个进程也可能把所有的资源吃光，而Linux Cgroups就是Linux内核中用来为进程设置资源限制的一个重要功能。 跟Namespace的情况类似，Cgroups对资源的限制能力也有很多不完善的地方，比如/proc文件系统的问题。 /proc目录存储的是记录当前内核运行状态的一系列特殊文件，用户可以通过访问这些文件，查看系统以及当前正在运行的进程的信息，比如CPU使用情况，内存占用率等。 当我们在容器内执行top命令的时候，会发现它显示的信息是宿主机的CPU和内存，而不是当前容器的数据。 rootfs：在Linux操作系统里，有一个名为chroot的命令，作用是帮助我们“change root file”，即改变进程的根目录到指定的位置。 而对于被chroot的进程来说，它并不知道自己的根目录已经被修改了。Mount Namespace就是基于chroot的不断改良被发明出来的。 为了让容器的根目录更加真实，一般会在这个容器的根目录下挂载一个完整操作系统的文件系统，比如Ubuntu 16.04的ISO。 这样，在容器启动之后，我们在容器里通过执行“ls /”查看根目录下的内容，就是Ubuntu 16.04的所有目录和文件。 这个挂载在容器根目录上，用来为容器进程提供隔离后执行环境的文件系统，就是容器镜像，专业名字rootfs(根文件系统)。 一个常见的rootfs，或者说容器镜像，会包含比如bin boot dev etc home lib lib64 media mnt opt proc root run sbin srv sys test tmp usr var等的目录和文件。 进入容器后执行的/bin/bash，就是/bin目录下的可执行文件，与宿主机的/bin/bash完全不同。 rootfs只是一个操作系统所包含的文件、配置和目录，并不包括操作系统内核。 所以说rootfs只包括了操作系统的“躯壳”，并没有包括操作系统的“灵魂”，同一台机器上的所有容器，都是共享宿主机操作系统的内核。 这意味着，容器进程需要配置内核参数、加载额外的内核模块，以及跟内核进行直接交互的时候，需要注意这些操作和依赖的对象，都是宿主机操作系统的内核，对该机器上的所有容器生效。 不过也真是由于rootfs，容器才有了一个被反复宣传至今的重要特性：一致性。 由于rootfs里打包的不只是应用，而是整个操作系统的文件和目录，也就意味着，应用以及它运行所需要的所有依赖，都被封装在了一起。 有了容器镜像“打包操作系统”的能力，这个最基础的依赖环境也变成了应用沙盒的一部分，这就赋予了容器的所谓一致性。 无论在本地、云端，还是在任何机器上，用户只需要解压打包好的容器镜像，那么这个应用运行所需要的完整的执行环境就被重现出来了。 这种深入到操作系统级别的运行环境一致性，打通了应用在本地开发和远端执行环境之间难以逾越的鸿沟。 另外，Docker在镜像的设计中，引入了层（layer）的概念，也就是说，用户制作镜像的每一步操作，都会生成一个层，也就是一个增量的rootfs。 这个设计用到了一种叫做联合文件系统（Union File System）的能力，也叫UnionFS,最主要的功能是将多个不同位置的目录联合挂载到同一个目录下。 如图所示，容器中的rootfs分为只读层、Init层、可读写层。 只读层：包含了操作系统的一部分。 Init层：夹在只读层和可读写层中间，是Docker项目单独生成的一个内部层，用来存在/etc/hosts、/etc/resolv.conf等信息。 可读写层：用来存放修改rootfs后产生的增量，无论增、删、改，都发生在这里。 使用docker commit和push指令保存并上传被修改过的容器的时候，只读层和Init层不会有任何变化，变化的只是可读写层。 Dockerfile实际操作中，我们不会使用docker commit命令来把一个运行中的docker容器提交成为一个docker镜像。 使用 docker commit 意味着所有对镜像的操作都是黑箱操作，生成的镜像也被称为黑箱镜像，换句话说，就是除了制作镜像的人知道执行过什么命令、怎么生成的镜像，别人根本无从得知。 另外结合之前的分层，任何修改的结果仅仅是在当前层进行标记、添加、修改，而不会改动上一层。 这样每一次修改都会让镜像更加臃肿一次，所删除的上一层的东西并不会丢失，会一直如影随形的跟着这个镜像，即使根本无法访问到。 把每一层修改、安装、构建、操作的命令都写入一个脚本，用这个脚本来构建、定制镜像，那么无法重复的问题、镜像构建透明性的问题、体积的问题就都会解决，这个脚本就是 Dockerfile。 Dockerfile 是一个文本文件，其内包含了一条条的指令(Instruction)，每一条指令构建一层，因此每一条指令的内容，就是描述该层应当如何构建。 每一个 RUN 的行为，就和手动docker commit建立镜像的过程一样：新建立一层，在其上执行这些命令，执行结束后，commit 这一层的修改，构成新的镜像。 Union FS 是有最大层数限制的，比如 AUFS，曾经是最大不得超过 42 层，现在是不得超过 127 层。 注本文内容是在极客时间学习张磊老师《深入剖析Kubernetes》专栏之后的一点理解，如果有写的不对的地方欢迎留言指正，另外也希望大家支持张磊老师，支持极客时间，支持知识付费，多多学习，共同进步。","categories":[{"name":"容器","slug":"容器","permalink":"http://yoursite.com/categories/容器/"}],"tags":[{"name":"docker","slug":"docker","permalink":"http://yoursite.com/tags/docker/"}]}]}