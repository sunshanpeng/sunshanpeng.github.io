{"meta":{"title":"阿鹏的博客","subtitle":null,"description":"java go devops kubernetes docker","author":"sunshanpeng","url":"http://sunshanpeng.com","root":"/"},"pages":[{"title":"","date":"2019-08-16T17:44:23.000Z","updated":"2019-08-16T18:15:30.107Z","comments":true,"path":"about/index.html","permalink":"http://sunshanpeng.com/about/index.html","excerpt":"","text":""},{"title":"All tags","date":"2018-12-22T04:39:04.000Z","updated":"2019-08-16T18:10:13.719Z","comments":true,"path":"tags/index.html","permalink":"http://sunshanpeng.com/tags/index.html","excerpt":"","text":""},{"title":"","date":"2019-08-16T17:42:27.000Z","updated":"2019-08-16T18:10:55.480Z","comments":true,"path":"categories/index.html","permalink":"http://sunshanpeng.com/categories/index.html","excerpt":"","text":""}],"posts":[{"title":"Kubernetes高可用部署","slug":"Kubernetes高可用架构","date":"2019-08-24T04:29:30.000Z","updated":"2019-08-25T10:38:36.660Z","comments":true,"path":"2019/08/24/Kubernetes高可用架构/","link":"","permalink":"http://sunshanpeng.com/2019/08/24/Kubernetes高可用架构/","excerpt":"","text":"基本架构 Kubernetes分为Master Node和Worker Node，其中Master Node作为控制节点，调度管理整个系统；Worker Node作为工作节点，运行业务容器。 Master Node api-server 作为kubernetes系统的入口，对外暴露了Kubernetes API，封装了核心对象的增删改查操作，以RESTFul接口方式提供给外部客户和内部组件调用。它维护的REST对象将持久化到etcd(一个分布式强一致性的key/value存储)。 scheduler 负责集群的资源调度，为新建的pod分配机器。这部分工作分出来变成一个组件，意味着可以很方便地替换成其他的调度器。 controller-manager 运行控制器，它们是处理集群中常规任务的后台线程。逻辑上，每个控制器是一个单独的进程，但为了降低复杂性，它们都被编译成独立的可执行文件，并在单个进程中运行。 这些控制器包括: 节点控制器: 当节点移除时，负责注意和响应。 副本控制器: 负责维护系统中每个副本控制器对象正确数量的 Pod。 端点控制器: 填充 端点(Endpoints) 对象(即连接 Services &amp; Pods)。 服务帐户和令牌控制器: 为新的命名空间创建默认帐户和 API 访问令牌 Worker Node kubeletkubelet是主要的节点代理,它监测已分配给其节点的 Pod(通过 apiserver 或通过本地配置文件)，提供如下功能: 挂载 Pod 所需要的数据卷(Volume)。 下载 Pod 的 secrets。 通过 Docker 运行(或通过其他容器运行时)运行 Pod 的容器。 周期性的对容器生命周期进行探测。 如果需要，通过创建 镜像 Pod（Mirror Pod） 将 Pod 的状态报告回系统的其余部分。将节点的状态报告回系统的其余部分。 kube-proxy通过维护主机上的网络规则并执行连接转发，实现了Kubernetes服务抽象。 存储Kubernetes 所有集群数据都存储在etcd里。 etcd通常部署在Master Node上，也可以单独部署。 官网地址：https://github.com/etcd-io/etcd 网络CNI（Container Network Interface）是CNCF旗下的一个项目，由一组用于配置Linux容器的网络接口的规范和库组成，同时还包含了一些插件。CNI仅关心容器创建时的网络分配，和当容器被删除时释放网络资源。 每个pod都会有一个集群内唯一的IP，不同Node上的Pod可以互相访问。 常用的网络插件有Calico和Flannel。 容器CRI（Container Runtime Interface）是容器运行时接口，kubelet可以运行实现了CRI的各种容器运行时而不仅仅是Docker。 高可用架构要实现高可用，就要实现所有Node没有单点故障，任何一台服务器宕机均不影响Kubernetes的正常工作。Kubernetes的高可用主要是Master Node和存储的高可用，包括以下几个组件： etcd属于CP架构，保证一致性和分区容错性。因为其内部使用 Raft 协议进行选举，所以需要部署基数个实例，比如3、5、7个。可以和Master Node一起部署也可以外置部署。 controller-manager和scheduler的高可用相对容易，是基于etcd的锁进行leader election。相当于基于etcd的分布式锁，谁拿到谁干活，同时只会有一个实例在工作，部署2个及以上的实例数量。 api-server是无状态的，需要部署2个及以上的实例数量，然后使用HAProxy和Keepalived作为负载均衡实现高可用。除了HAProxy也可以使用Nginx或其他负载均衡策略。 方案一 使用三台4核8G的服务器当Master Node，上面部署api-server、scheduler、controller-manager、kubelet、kube-proxy、docker、calico、etcd、HAProxy、keepalived。 Worker Node的配置根据实际情况，最好CPU和内存的大小、比例都一样。上面部署kubelet、kube-proxy、docker、calico。 该方案集群内外连接api-server都是高可用，使用方便。但是公有云不能用虚拟IP所以不能用该方案。另外所有的网络请求都会集中到一台服务器（虚拟IP所在的服务器）进行转发，负载不均衡，HAProxy的上限即api-server的上限。 方案二 使用三台4核8G的服务器当Master Node，上面部署api-server、scheduler、controller-manager、kubelet、kube-proxy、docker、calico、etcd。 Worker Node的配置根据实际情况，最好CPU和内存的大小、比例都一样。上面部署kubelet、kube-proxy、docker、calico、HAProxy。 该方案兼容自有机房和公有云，Master Node负载比较均衡。但是集群外访问api-server没有做到高可用，需要额外解决。如果Worker Node上的HAProxy宕机则该节点不能正常工作。另外如果添加和删除Master Node需要更新所有Worker Node的负载均衡配置（可选项，不修改也可以）。 部署方式二进制部署所有组件均使用二进制文件部署，手动部署可以参考：https://github.com/opsnull/follow-me-install-kubernetes-cluster；ansible脚本部署可以用：https://github.com/easzlab/kubeasz。 容器化部署kubernetes的所有组件中，除了docker和kubelet是运行和管理容器的，其他组件都可以使用容器化的方式部署，最流行的方式就是使用kubeadm。 官网地址：https://github.com/kubernetes/kubeadm。 参考https://kubernetes.io/zh/docs/concepts/overview/components/ https://www.kubernetes.org.cn/2931.html https://jimmysong.io/kubernetes-handbook/concepts/cni.html https://jimmysong.io/kubernetes-handbook/concepts/cri.html","categories":[{"name":"容器","slug":"容器","permalink":"http://sunshanpeng.com/categories/容器/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"http://sunshanpeng.com/tags/k8s/"}]},{"title":"Docker常用命令","slug":"Docker常用命令","date":"2019-03-24T14:26:37.000Z","updated":"2019-08-18T11:21:45.249Z","comments":true,"path":"2019/03/24/Docker常用命令/","link":"","permalink":"http://sunshanpeng.com/2019/03/24/Docker常用命令/","excerpt":"","text":"拉取镜像1docker pull mysql:5.6 格式：docker pull [OPTIONS] NAME[:TAG|@DIGEST]，tag不写的话默认latest。 登录镜像仓库1docker login --username sunshanpeng --password 123456 harbor.sunshanpeng.com 格式：docker login [OPTIONS] [SERVER] SERVER为空的情况下默认登录hub.docker.com password建议不要显示在控制台上 列出所有镜像1docker images 格式：docker images [OPTIONS] [REPOSITORY[:TAG]]，可选参数显示全部镜像或者过滤镜像。 列出所有容器1docker ps -a 格式：docker ps [OPTIONS]，去掉-a显示运行中的容器。 给镜像打tag1docker tag mysql:5.6 harbor.sunshanpeng.com/database/mysql:5.6 格式：docker tag SOURCE_IMAGE[:TAG] TARGET_IMAGE[:TAG] 当我们从公共仓库拉取的镜像，想推送到我们自己搭建的私有仓库，需要先打一个tag； 当我们拉取不到一些需要翻墙才能拉取的镜像时，也可以从国内镜像仓库拉取镜像然后打个tag给成目标镜像。 推送镜像1docker push harbor.sunshanpeng.com/database/mysql:5.6 格式：docker push [OPTIONS] NAME[:TAG] 推送镜像必须先登录要推送的目标仓库，而且需要对目标仓库有推送的权限。 类似docker push mysql:5.6是不行的。 制作镜像1docker build -t simpleApp:v1 . 格式：docker build [OPTIONS] PATH | URL | -。 根据Dockerfile制作镜像。 删除镜像1docker rmi mysql:5.6 格式：docker rmi [OPTIONS] IMAGE [IMAGE...]，不能删除已经创建了容器的镜像，必须先删除对应容器。 运行容器1docker run nginx 格式：docker run [OPTIONS] IMAGE [COMMAND] [ARG...] OPTIONS参数有很多，比较常用的如下： --name指定容器名字 -p指定容器映射宿主机的端口号 -e指定环境变量 -v指定挂载卷 -it指定容器前台运行 -d指定容器后台运行 其他还有很多指定IP、Hostname、CPU、内存、网络、内核参数、重启策略等等参数，可以通过docker run --help查询。 mysql容器启动命令1docker run --name mysql -p 3306:3306 -e MYSQL_ROOT_PASSWORD=123456 -v=/docker/mysql/lib:/var/lib/mysql -v=/docker/mysql/my.cnf:/etc/mysql/my.cnf -v=/usr/share/zoneinfo/Asia/Shanghai:/etc/localtime -d mysql:5.6 redis容器启动命令1docker run --name redis -p 6379:6379 /docker/redis/data:/data -d redis:3.2 重命名容器1docker rename mysql mysql1 格式：docker rename CONTAINER NEW_NAME。 停止容器1docker stop mysql 格式：docker stop [OPTIONS] CONTAINER [CONTAINER...]，停止多个容器用空格分隔容器名字或者容器id。 强制停止容器1docker kill mysql 格式：docker kill [OPTIONS] CONTAINER [CONTAINER...]。 启动/重启容器1docker start/restart mysql 格式：docker start/restart [OPTIONS] CONTAINER [CONTAINER...]。 删除容器1docker rm mysql 格式：docker rm [OPTIONS] CONTAINER [CONTAINER...]，不带参数时只能删除已停止的容器，带上参数-f可以强制删除运行中的容器。 更新容器1docker update --restart always mysql 格式：docker update [OPTIONS] CONTAINER [CONTAINER...]。 更新容器的重启策略、CPU和内存大小。 进入容器1docker exec -it mysql /bin/bash 格式：docker exec [OPTIONS] CONTAINER COMMAND [ARG...]。 CONTAINER 可以是容器Id或者容器名字； /bin/bash是进入容器后执行的命令，有些容器可能不存在/bin/bash。 导出镜像文件1docker save -o rook-ceph-093.tar rook/ceph:v0.9.3 格式：docker save [OPTIONS] IMAGE [IMAGE...]。 如果同时导出多个镜像到一个文件里，后面的镜像列表用空格来分隔。 导入镜像文件1docker load -i /root/rook-ceph-093.tar 格式：docker load [OPTIONS] 当不方便使用自建的镜像仓库来传输镜像时，可以用镜像的导出导入功能来移动镜像。 查看容器资源消耗123456docker statsCONTAINER ID NAME CPU % MEM USAGE / LIMIT MEM % NET I/O BLOCK I/O PIDS63ebc0e88e5a happy_volhard 0.02% 35.3MiB / 15.46GiB 0.22% 11.5MB / 19MB 13.3MB / 45MB 227ed66f584a25 redis 0.08% 7.82MiB / 15.46GiB 0.05% 304kB / 274kB 5.59MB / 0B 3500eed21d914 dockercompose_mqbroker_1 2.98% 752.3MiB / 15.46GiB 4.75% 0B / 0B 3.07GB / 19GB 204ddd919a5bf52 dockercompose_mqnamesrv_1 0.21% 329.2MiB / 15.46GiB 2.08% 0B / 0B 60.3MB / 16.4kB 41 查看Docker占用磁盘1234567docker system dfTYPE TOTAL ACTIVE SIZE RECLAIMABLEImages 29 28 8.575GB 1.873GB (21%)Containers 29 17 7.545GB 410.8MB (5%)Local Volumes 303 23 6.339GB 4.138GB (65%)Build Cache 0 0 0B 0B 可以查看镜像文件大小、容器大小、本地存储使用的磁盘大小。 清理Docker镜像和容器1docker system prune -a 此命令会清除所有未运行的容器和清理所有未使用的镜像。","categories":[{"name":"容器","slug":"容器","permalink":"http://sunshanpeng.com/categories/容器/"}],"tags":[{"name":"docker","slug":"docker","permalink":"http://sunshanpeng.com/tags/docker/"}]},{"title":"Kubernetes集群配置优化","slug":"Kubernetes集群配置优化","date":"2019-03-24T14:26:37.000Z","updated":"2019-08-16T18:20:56.314Z","comments":true,"path":"2019/03/24/Kubernetes集群配置优化/","link":"","permalink":"http://sunshanpeng.com/2019/03/24/Kubernetes集群配置优化/","excerpt":"","text":"一、Docker配置123456789101112131415&#123; \"exec-opts\": [\"native.cgroupdriver=systemd\"], \"log-driver\": \"json-file\", \"log-opts\": &#123; \"max-size\": \"100m\", \"max-file\": \"10\" &#125;, \"bip\": \"169.254.123.1/24\", \"oom-score-adjust\": -1000, \"registry-mirrors\": [\"https://registry.docker-cn.com\", \"https://docker.mirrors.ustc.edu.cn\"], \"storage-driver\": \"overlay2\", \"storage-opts\":[\"overlay2.override_kernel_check=true\"], \"data-root\": \"/var/lib/docker\", \"live-restore\": true&#125; data-root设置存储持久数据和资源配置的目录，默认值/var/lib/docker，17.05.0以下版本参数为：graph。 如果只有一个系统盘可以不用指定该参数，如果有数据盘并且数据盘不是/var，则需要指定数据目录。 registry-mirrors镜像仓库地址，用于镜像加速。 直接连接国外dockerHub拉取镜像速度太慢，一般都会配置国内的镜像地址。 bip设置docker0网桥地址，默认是172.17.0.1/16。 默认网段有可能和现有服务器的网段冲突，建议设置成169.254.123.1/24 live-restore当docker daemon停止时，让容器继续运行，默认false关闭，设为true打开。 默认情况下重启或者关闭docker daemon时，容器都会停止，通过设置live-restore让容器继续运行，在修改docker配置或者升级docker的时候很有用。（如果daemon重启之后使用了不同的bip或不同的data-root，则live restore无法正常工作） log-optsdocker日志设置，默认为空。 不设置最大值的话日志文件可能会撑爆硬盘。 exec-opts设置docker cgroup驱动。 需要注意docker的Cgroups和kubelet的Cgroups配置是否一致。 二、Kubelet配置123456789101112131415161718192021222324--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --pod-manifest-path=/etc/kubernetes/manifests --allow-privileged=true --network-plugin=cni --cni-conf-dir=/etc/cni/net.d --cni-bin-dir=/opt/cni/bin --cluster-dns=192.168.176.10 --pod-infra-container-image=registry-vpc.cn-shanghai.aliyuncs.com/acs/pause-amd64:3.1 --enable-controller-attach-detach=false --enable-load-reader --cluster-domain=cluster.local --cloud-provider=external --hostname-override=$(NODE_NAME)--provider-id=cn-shanghai.i-uf657oy682jirk1oad62 --authorization-mode=Webhook --client-ca-file=/etc/kubernetes/pki/ca.crt --system-reserved=memory=300Mi --kube-reserved=memory=400Mi --eviction-hard=imagefs.available&lt;15%,memory.available&lt;300Mi,nodefs.available&lt;10%,nodefs.inodesFree&lt;5% --cgroup-driver=systemd --anonymous-auth=false --rotate-certificates=true --cert-dir=/var/lib/kubelet/pki--root-dir=/var/lib/kubelet pod-infra-container-image基础镜像，默认使用的k8s.gcr.io/pause-amd64:3.1需要翻墙，可以使用国内镜像或者自己上传的镜像。 root-dir设置存储持久数据和资源配置的目录，默认值/var/lib/kubelet。 用法同Docker的data-root参数。 cgroup-driver设置kubelet Cgroups驱动。 需要注意docker的Cgroups和kubelet的Cgroups配置是否一致。 system-reserved给系统保留的资源大小，可以设置CPU、内存、硬盘。 kube-reserved给kubernetes保留的资源大小，可以设置CPU、内存、硬盘。 eviction-hard驱逐条件，当节点资源小于设置的驱逐条件时，kubelet开始驱逐Pod。 三、Kube-proxy1234- --proxy-mode=ipvs- --kubeconfig=/var/lib/kube-proxy/kubeconfig.conf- --cluster-cidr=172.17.0.0/18- --hostname-override=$(NODE_NAME) proxy-modeipvs可以大大提高性能。 参考资料： https://docs.docker.com/engine/reference/commandline/dockerd/ https://kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources/ https://kubernetes.io/docs/tasks/administer-cluster/out-of-resource/ https://docs.docker.com/engine/deprecated/","categories":[{"name":"容器","slug":"容器","permalink":"http://sunshanpeng.com/categories/容器/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"http://sunshanpeng.com/tags/k8s/"},{"name":"docker","slug":"docker","permalink":"http://sunshanpeng.com/tags/docker/"}]},{"title":"Windows电脑本地开发时连接Apollo配置中心启动慢","slug":"Windows电脑本地开发时连接Apollo配置中心启动慢","date":"2019-03-24T14:26:37.000Z","updated":"2019-08-18T03:58:48.733Z","comments":true,"path":"2019/03/24/Windows电脑本地开发时连接Apollo配置中心启动慢/","link":"","permalink":"http://sunshanpeng.com/2019/03/24/Windows电脑本地开发时连接Apollo配置中心启动慢/","excerpt":"","text":"表现：启动时中间停顿了10秒左右 原因：电脑上网卡太多 解决方法： 禁用多余网卡 在启动参数指定本机IP：-Dhost.ip=10.208.202.251 10.208.202.251改成自己的IP地址 原理：指定本机IP后就会跳过查找有效IP这一步 具体代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125package com.ctrip.framework.foundation.internals;import java.net.Inet4Address;import java.net.InetAddress;import java.net.NetworkInterface;import java.net.SocketException;import java.net.UnknownHostException;import java.util.ArrayList;import java.util.Collections;import java.util.Enumeration;import java.util.List;import java.util.Objects;public enum NetworkInterfaceManager &#123; INSTANCE; private InetAddress m_local; private InetAddress m_localHost; private NetworkInterfaceManager() &#123; load(); &#125; public InetAddress findValidateIp(List&lt;InetAddress&gt; addresses) &#123; InetAddress local = null; int maxWeight = -1; for (InetAddress address : addresses) &#123; if (address instanceof Inet4Address) &#123; int weight = 0; if (address.isSiteLocalAddress()) &#123; weight += 8; &#125; if (address.isLinkLocalAddress()) &#123; weight += 4; &#125; if (address.isLoopbackAddress()) &#123; weight += 2; &#125; // has host name // TODO fix performance issue when calling getHostName if (!Objects.equals(address.getHostName(), address.getHostAddress())) &#123; weight += 1; &#125; if (weight &gt; maxWeight) &#123; maxWeight = weight; local = address; &#125; &#125; &#125; return local; &#125; public String getLocalHostAddress() &#123; return m_local.getHostAddress(); &#125; public String getLocalHostName() &#123; try &#123; if (null == m_localHost) &#123; m_localHost = InetAddress.getLocalHost(); &#125; return m_localHost.getHostName(); &#125; catch (UnknownHostException e) &#123; return m_local.getHostName(); &#125; &#125; private String getProperty(String name) &#123; String value = null; value = System.getProperty(name); if (value == null) &#123; value = System.getenv(name); &#125; return value; &#125; private void load() &#123; String ip = getProperty(\"host.ip\");//除了启动参数也可以在环境变量中指定host.ip if (ip != null) &#123;//如果指定了host.ip就直接拿指定IP的网卡信息 try &#123; m_local = InetAddress.getByName(ip); return; &#125; catch (Exception e) &#123; System.err.println(e); // ignore &#125; &#125; try &#123; Enumeration&lt;NetworkInterface&gt; interfaces = NetworkInterface.getNetworkInterfaces(); List&lt;NetworkInterface&gt; nis = interfaces == null ? Collections.&lt;NetworkInterface&gt;emptyList() : Collections.list(interfaces); List&lt;InetAddress&gt; addresses = new ArrayList&lt;InetAddress&gt;(); InetAddress local = null; try &#123; for (NetworkInterface ni : nis) &#123; if (ni.isUp() &amp;&amp; !ni.isLoopback()) &#123; addresses.addAll(Collections.list(ni.getInetAddresses())); &#125; &#125; local = findValidateIp(addresses); &#125; catch (Exception e) &#123; // ignore &#125; if (local != null) &#123; m_local = local; return; &#125; &#125; catch (SocketException e) &#123; // ignore it &#125; m_local = InetAddress.getLoopbackAddress(); &#125;&#125; 注： 官方说还可以通过改host的方式实现，但是我在win10电脑上没有成功。 1.4修复了该问题 参考链接：https://github.com/ctripcorp/apollo/wiki/%E9%83%A8%E7%BD%B2&amp;%E5%BC%80%E5%8F%91%E9%81%87%E5%88%B0%E7%9A%84%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98#6-%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%A4%9A%E5%9D%97%E7%BD%91%E5%8D%A1%E9%80%A0%E6%88%90%E8%8E%B7%E5%8F%96ip%E4%B8%8D%E5%87%86%E5%A6%82%E4%BD%95%E8%A7%A3%E5%86%B3 https://github.com/ctripcorp/apollo/issues/1457","categories":[{"name":"java","slug":"java","permalink":"http://sunshanpeng.com/categories/java/"}],"tags":[{"name":"apollo","slug":"apollo","permalink":"http://sunshanpeng.com/tags/apollo/"}]},{"title":"Kubernetes介绍","slug":"Kubernetes介绍","date":"2019-02-24T14:52:37.000Z","updated":"2019-08-16T18:21:22.028Z","comments":true,"path":"2019/02/24/Kubernetes介绍/","link":"","permalink":"http://sunshanpeng.com/2019/02/24/Kubernetes介绍/","excerpt":"","text":"有了Docker之后，单个应用进程的使用管理变的更简单了，但是如果想要管理一个集群的不同应用，处理应用之间的关系， 比如：一个Web应用与数据库之间的访问关系，负载均衡和后端服务之间的代理关系，门户应用与授权组件的调用关系， 再比如说一个服务单位的不同功能之间的关系，如一个Web应用与日志搜集组件之间的文件交换关系， 在传统虚拟机环境对这种关系的处理方法都比较“粗颗粒”，一股脑的部署在同一台虚拟机中，需要手动维护很多跟应用协作的守护进程，用来处理它的日志搜集、灾难恢复、数据备份等辅助工作。 使用容器和编排技术以后，那些原来挤在同一个虚拟机里的各个应用、组件、守护进程，都可以被分别做成镜像，然后运行在一个个专属的容器中。 它们之间互不干涉，拥有各自的资源配额，可以被调度在整个集群里的任何一台机器上。 如果只是用来封装微服务、拉取用户镜像、调度单容器，用Docker Swarm就足够了，而且方便有效，但是如果需要路由网关、水平扩展、弹性伸缩、监控、备份、灾难恢复等一系列运维能力，就需要使用kubernetes来解决了。 Kubernetes介绍 Kubernetes是Google开源的容器编排调度引擎，它的目标不仅仅是一个编排系统，而是提供一个规范，可以用来描述集群的架构，定义服务的最终状态，Kubernetes可以将系统自动得到和维持在这个状态。 更直白的说，Kubernetes用户可以通过编写一个yaml或者json格式的配置文件，也可以通过工具/代码生成或直接请求Kubernetes API创建应用，该配置文件中包含了用户想要应用程序保持的状态，不论整个Kubernetes集群中的个别主机发生什么问题，都不会影响应用程序的状态，你还可以通过改变该配置文件或请求Kubernetes API来改变应用程序的状态。 配置文件里面怎么定义的，整个系统就是怎么运行的。 kubernetes 的马斯洛需求 Kubernetes架构图 Kubernets属于主从的分布式集群架构，包含Master和Node： Master作为控制节点，调度管理整个系统，包含以下组件： API Server作为kubernetes系统的入口，封装了核心对象的增删改查操作，以RESTFul接口方式提供给外部客户和内部组件调用。它维护的REST对象将持久化到etcd(一个分布式强一致性的key/value存储)。 Scheduler：负责集群的资源调度，为新建的pod分配机器。这部分工作分出来变成一个组件，意味着可以很方便地替换成其他的调度器。 Controller Manager：负责执行各种控制器，目前有两类：（1）Endpoint Controller：定期关联service和pod(关联信息由endpoint对象维护)，保证service到pod的映射总是最新的。（2）Replication Controller：定期关联replicationController和pod，保证replicationController定义的复制数量与实际运行pod的数量总是一致的。 Node是运行节点，运行业务容器，包含以下组件： Kubelet：负责管控docker容器，如启动/停止、监控运行状态等。它会定期从etcd获取分配到本机的pod，并根据pod信息启动或停止相应的容器。同时，它也会接收apiserver的HTTP请求，汇报pod的运行状态。 Kube Proxy：负责为pod提供代理。它会定期从etcd获取所有的service，并根据service信息创建代理。当某个客户pod要访问其他pod时，访问请求会经过本机proxy做转发。 Kubernets使用Etcd作为存储和通信中间件，实现Master和Node的一致性，这是目前比较常见的做法，典型的SOA架构，解耦Master和Node。 基本概念 PodPod是Kubernetes的基本操作单元，把相关的一个或多个容器构成一个Pod，通常Pod里的容器运行相同的应用。Pod包含的容器运行在同一个Node(Host)上，看作一个统一管理单元，共享相同的volumes和network namespace/IP和Port空间。 Replication ControllerReplication Controller确保任何时候Kubernetes集群中有指定数量的pod副本(replicas)在运行， 如果少于指定数量的pod副本(replicas)，Replication Controller会启动新的Container，反之会杀死多余的以保证数量不变。 Replication Controller使用预先定义的pod模板创建pods，一旦创建成功，pod 模板和创建的pods没有任何关联，可以修改pod 模板而不会对已创建pods有任何影响，也可以直接更新通过Replication Controller创建的pods。 对于利用pod 模板创建的pods，Replication Controller根据label selector来关联，通过修改pods的label可以删除对应的pods。 ServiceService也是Kubernetes的基本操作单元，是真实应用服务的抽象，每一个服务后面都有很多对应的容器来支持，通过Proxy的port和服务selector决定服务请求传递给后端提供服务的容器，对外表现为一个单一访问接口，外部不需要了解后端如何运行，这给扩展或维护后端带来很大的好处。 LabelLabel是用于区分Pod、Service、Replication Controller的key/value键值对，Pod、Service、 Replication Controller可以有多个label，但是每个label的key只能对应一个value。Labels是Service和Replication Controller运行的基础，为了将访问Service的请求转发给后端提供服务的多个容器，正是通过标识容器的labels来选择正确的容器。同样，Replication Controller也使用labels来管理通过pod 模板创建的一组容器，这样Replication Controller可以更加容易，方便地管理多个容器，无论有多少容器。","categories":[{"name":"容器","slug":"容器","permalink":"http://sunshanpeng.com/categories/容器/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"http://sunshanpeng.com/tags/k8s/"}]},{"title":"Pod的意义","slug":"Pod的意义","date":"2019-02-24T14:29:37.000Z","updated":"2019-08-16T18:21:54.305Z","comments":true,"path":"2019/02/24/Pod的意义/","link":"","permalink":"http://sunshanpeng.com/2019/02/24/Pod的意义/","excerpt":"","text":"Kubernetes 使用 Pod 来管理容器，每个 Pod 可以包含一个或多个紧密关联的容器。 Pod 是一组紧密关联的容器集合，它们共享 PID、IPC、Network 和 UTS namespace，是 Kubernetes 调度的基本单位。 Pod 内的多个容器共享网络和文件系统，可以通过进程间通信和文件共享这种简单高效的方式组合完成服务。 Pod的意义容器都是单进程运行的，不是因为容器只能运行一个进程，而是容器没有管理多个进程的能力。 容器里PID=1的进程就是应用本身，其他的进程都是这个PID=1进程的子进程。如果启动了第二个进程，那这第二个进程异常退出的时候，外部并不能感知，也就管理不了。 在一个真正的操作系统里，进程并不是“孤苦伶仃”地独自运行的，应用之间可能有着密切的协作关系，必须部署在同一台机器上。 这些密切关系包括但不限于：互相之间会发生直接的文件交换，使用localhost或者Socket文件进行本地通信，会发生非常频繁的远程调用，共享某些Linux Namespace。 Pod的实现原理首先，Pod只是一个逻辑概念，是一组共享了某些资源的容器。 Kubernetes真正处理的，还是宿主机操作系统上Linux容器的Namespace和Cgroups，而并不存在一个所谓的Pod的边界或者隔离环境。 Kubernetes里，Pod的实现需要使用一个中间容器，叫做Infra容器，在Pod中，Infra容器永远都是第一个被创建的容器。 其他用户自定义的容器，通过Join Network Namespace的方式与Infra容器关联在一起。 如图所示，这个Pod里除了两个用户容器，还有一个Infra容器。 这个Infra容器占用极少的资源，使用了一个非常特殊的镜像：k8s.gcr.io/pause，这个镜像永远处于“暂停”状态，100-200KB左右大小。 Infra容器生成NetWork Namespace后，用户容器加入Infra容器的Network Namespace中，用户容器和Infra容器在宿主机上的Namespace文件是一样的。 这意味着，对Pod中的容器A和容器B来说： 它们可以直接使用localhost进行通信； 他们看到的网络设备和Infra容器的一样； 一个Pod只有一个IP地址，也就是容器A、容器B、Infra容器的ip地址一致； 所有的网络资源被改Pod中的所有容器共享； Pod的生命周期只跟Infra容器一致，与容器A和B无关。 对于同一个Pod里面的所有用户容器来说，它们的进出流量，都是通过Infra容器完成的。 Pod的本质，实际上是在扮演传统基础设施“虚拟机”的角色；容器则是这个虚拟机里运行的用户进程。","categories":[{"name":"容器","slug":"容器","permalink":"http://sunshanpeng.com/categories/容器/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"http://sunshanpeng.com/tags/k8s/"}]},{"title":"使用apollo配置中心生成雪花算法的workerId","slug":"使用apollo配置中心生成雪花算法的workerId","date":"2019-02-24T14:29:37.000Z","updated":"2019-08-18T05:28:26.124Z","comments":true,"path":"2019/02/24/使用apollo配置中心生成雪花算法的workerId/","link":"","permalink":"http://sunshanpeng.com/2019/02/24/使用apollo配置中心生成雪花算法的workerId/","excerpt":"","text":"前言在分布式系统中，有一些需要使用全局唯一ID的场景，这时候雪花算法（snowflake）就是一种不错的解决方式。 但是雪花算法需要用到一个workerId，同一个应用部署多个实例的时候workerId不能相同。 使用环境变量或者启动参数来指定workerId的方式虽然在一定程度上解决了workerId的问题，但是给容器环境下的自动化部署或者动态扩容带来了新的挑战。 Apollo配置中心介绍Apollo（阿波罗）是携程框架部门研发的分布式配置中心，能够集中化管理应用不同环境、不同集群的配置，配置修改后能够实时推送到应用端，并且具备规范的权限、流程治理等特性，适用于微服务配置管理场景。 GitHub地址：https://github.com/ctripcorp/apollo Apollo分为Client、Config Service、Admin Service、Portal四个部分： Client是一个jar包集成在业务应用里，通过Meta Server从Config Service获取配置信息； Config Service提供配置的读取、推送等功能，服务对象是Apollo客户端（Client）； Admin Service提供配置的修改、发布等功能，服务对象是Apollo管理界面（Portal）； Portal通过Meta Server连接Admin Service修改、发布配置信息。 ConfigDB存储配置信息，由Config Service和Admin Service使用； PortalDB存储权限和审计信息，由Portal使用。 改造Config Service生成workerId原理介绍Client端通过/configs/{appId}/{clusterName}/{namespace:.+}接口拉取全量配置信息，对应方法是com.ctrip.framework.apollo.configservice.controller.ConfigController#queryConfig。方法的返回值ApolloConfig有5个属性：appId、cluster、namespaceName、configurations、releaseKey,只要把生成的workerId放到configurations这个Map类型的属性中，Client端就能直接通过Key值拿到workerId来使用。 具体实现一、数据库改动ConfigDB库的instance表存储了使用配置的应用实例。 在Instance表新加一个NodeId字段用来存储生成的workerId，并且将AppId和NodeId设置为唯一索引：123456789101112131415CREATE TABLE `instance` ( `Id` int(11) unsigned NOT NULL AUTO_INCREMENT COMMENT &apos;自增Id&apos;, `AppId` varchar(32) NOT NULL DEFAULT &apos;default&apos; COMMENT &apos;AppID&apos;, `ClusterName` varchar(32) NOT NULL DEFAULT &apos;default&apos; COMMENT &apos;ClusterName&apos;, `DataCenter` varchar(64) NOT NULL DEFAULT &apos;default&apos; COMMENT &apos;Data Center Name&apos;, `Ip` varchar(32) NOT NULL DEFAULT &apos;&apos; COMMENT &apos;instance ip&apos;, `NodeId` int(5) DEFAULT NULL COMMENT &apos;节点id&apos;, `DataChange_CreatedTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT &apos;创建时间&apos;, `DataChange_LastTime` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT &apos;最后修改时间&apos;, PRIMARY KEY (`Id`), UNIQUE KEY `IX_UNIQUE_KEY` (`AppId`,`ClusterName`,`Ip`,`DataCenter`), UNIQUE KEY `uk_appId_nodeId` (`AppId`,`NodeId`) USING BTREE, KEY `IX_IP` (`Ip`), KEY `IX_DataChange_LastTime` (`DataChange_LastTime`)) ENGINE=InnoDB AUTO_INCREMENT=6779 DEFAULT CHARSET=utf8mb4 COMMENT=&apos;使用配置的应用实例&apos;; 二、代码改动原来Instance表的记录是在获取配置时调用com.ctrip.framework.apollo.configservice.util.InstanceConfigAuditUtil#audit异步添加的，这里需要改成获取配置时同步添加实例信息并且放到ApolloConfig的configurations属性中。 1.com.ctrip.framework.apollo.configservice.controller.ConfigController queryConfig方法返回前获取实例信息并返回123456Instance instance = instanceService.findInstance(appId, clusterName, dataCenter, clientIp);Integer nodeId = instance.getNodeId();if (nodeId == null) &#123; nodeId = -1;&#125;apolloConfig.addConfig(\"apollo.node.id\", String.valueOf(nodeId)); 2.com.ctrip.framework.apollo.biz.service.InstanceService12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879@Value(\"$&#123;apollo.node.minId&#125;\")//workerId最小值，0-1023之间private Integer minId;@Value(\"$&#123;apollo.node.maxId&#125;\")//workerId最大值，0-1023之间private Integer maxId;private static final Integer DEFAULT_NODE_ID = null;public Instance findInstance(String appId, String clusterName, String dataCenter, String ip) &#123; Instance instance = instanceRepository.findByAppIdAndClusterNameAndDataCenterAndIp(appId, clusterName, dataCenter, ip); if (instance == null) &#123; instance = createInstance(appId, clusterName, dataCenter, ip); &#125; if (instance.getNodeId() == null) &#123; updateInstance(instance); &#125; return instance;&#125;private Instance createInstance(String appId, String clusterName, String dataCenter, String ip) &#123; Instance instance = new Instance(); instance.setAppId(appId); instance.setClusterName(clusterName); instance.setDataCenter(dataCenter); instance.setIp(ip); instance.setNodeId(generateNodeId(appId)); try &#123; instance = createInstance(instance); &#125; catch (DataIntegrityViolationException e) &#123; //使用数据库的唯一约束来保证workerId的唯一性 logger.error(\"createInstance.error,instance=&#123;&#125;\", instance); instance = findInstance(appId, clusterName, dataCenter, ip); if (instance == null) &#123; instance = createInstance(appId, clusterName, dataCenter, ip); &#125; &#125; return instance;&#125;private void updateInstance(Instance instance) &#123; try &#123; instance.setNodeId(generateNodeId(instance.getAppId())); instanceRepository.save(instance); &#125; catch (DataIntegrityViolationException e) &#123; logger.error(\"updateInstance.error,instance=&#123;&#125;\", instance); updateInstance(instance); &#125;&#125;private Integer generateNodeId(String appId) &#123; //生成规则是当前appId最大workerId + 1 //如果workerId已经到最大值了就拿最早的workerId（这里假设一个应用不会同时有1023（maxId）个实例在运行） Integer maxNodeId = instanceRepository.maxNodeIdByAppId(appId); if (maxNodeId == null || minId &gt; maxNodeId) &#123; return minId; &#125; if (maxNodeId &lt; maxId) &#123; return maxNodeId + 1; &#125; return getEarliestNodeId(appId);&#125; private Integer getEarliestNodeId(String appId) &#123; InstanceConfig instanceConfig = instanceConfigRepository.findTopByConfigAppIdOrderByReleaseDeliveryTime(appId); if (instanceConfig == null) &#123; return DEFAULT_NODE_ID; &#125; Instance instance = instanceRepository.findOne(instanceConfig.getInstanceId()); if (instance == null) &#123; return DEFAULT_NODE_ID; &#125; Integer nodeId = instance.getNodeId(); instance.setNodeId(null); instanceRepository.save(instance); return nodeId; &#125; 三、使用方式和其他配置的使用方式一样，直接用@Value(&quot;${apollo.node.id}&quot;)。 这种方式生成的workerId可以直接使用在只需要一个workerId参数的雪花算法，另一种需要datacenterId和workerId两个参数的雪花算法在使用时需要做一下转换：12datacenterId=apollo.node.id &gt;&gt; 5workerId=apolo.node.id &amp; 31 取高五位的值作为datacenterId，取低五位的值作为workerId。 一个workerId参数的雪花算法，workerId最大1023，二进制为1111111111； datacenterId和workerId两个参数的雪花算法，datacenterId最大31，二进制为11111，workerId最大31，二进制为11111，datacenterId作为二进制的高5位，workerId作为二进制的低5位，组合起来1111111111，十进制为1023。","categories":[{"name":"java","slug":"java","permalink":"http://sunshanpeng.com/categories/java/"}],"tags":[{"name":"apollo","slug":"apollo","permalink":"http://sunshanpeng.com/tags/apollo/"}]},{"title":"修改Kubernetes集群Pod容器的内核参数","slug":"修改Kubernetes集群Pod容器的内核参数","date":"2019-02-24T14:29:37.000Z","updated":"2019-08-16T18:26:30.880Z","comments":true,"path":"2019/02/24/修改Kubernetes集群Pod容器的内核参数/","link":"","permalink":"http://sunshanpeng.com/2019/02/24/修改Kubernetes集群Pod容器的内核参数/","excerpt":"","text":"容器的本质是一个进程，共享Node的内核。原以为修改了Node的内核参数容器中也会改，但实际上并不是这样，容器的内核参数可以和Node不同。 Docker Daemon1docker run -it --rm --sysctl net.core.somaxconn=65535 busybox cat /proc/sys/net/core/somaxconn 多个参数： 12345678910111213docker run -itd --restart=always --net=host \\--name=centos01 --hostname=centos01 \\--sysctl kernel.msgmnb=13107200 \\--sysctl kernel.msgmni=256 \\--sysctl kernel.msgmax=65536 \\--sysctl kernel.shmmax=69719476736 \\--sysctl kernel.sem='500 256000 250 1024' \\-v /mnt:/update \\centos /bin/bash docker exec centos01 sysctl -a |grep -E \\'kernel.msgmnb|kernel.msgmni|kernel.msgmax|kernel.shmmax|kernel.sem' KubernetesKubernetes Sysctls 增加Kubelet启动参数 1kubelet --allowed-unsafe-sysctls=net.* 重新加载配置 1systemctl daemon-reload 重启kubelet 1systemctl restart kubelet 配置Pod的securityContext参数（注意是pod不是container） 1234securityContext: sysctls: - name: net.ipv4.tcp_keepalive_time value: &quot;180&quot; Kubernetes允许配置的内核参数如下：12345kernel.shm*,kernel.msg*,kernel.sem,fs.mqueue.*,net.*. 使用Kubernetes Sysctls推荐指定几台Node，利用污点或者节点亲和性把需要修改内核参数的pod调度到指定节点，而不是修改所有Node。 Kubernetes Init Container使用init container不需要修改kubelet参数123456789initContainers:- command: - sysctl - -w - net.ipv4.tcp_keepalive_time=180 image: busybox:1.27 name: init-sysctl securityContext: privileged: true 参考资料 https://www.cnblogs.com/DaweiJ/articles/8528687.html https://yq.aliyun.com/articles/603745?utm_content=m_1000003707 https://kubernetes.io/docs/tasks/administer-cluster/sysctl-cluster/","categories":[{"name":"容器","slug":"容器","permalink":"http://sunshanpeng.com/categories/容器/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"http://sunshanpeng.com/tags/k8s/"}]},{"title":"使用同一个deployment实现金丝雀（灰度）发布","slug":"使用同一个deployment实现金丝雀（灰度）发布","date":"2019-02-24T14:29:37.000Z","updated":"2019-08-16T18:32:04.071Z","comments":true,"path":"2019/02/24/使用同一个deployment实现金丝雀（灰度）发布/","link":"","permalink":"http://sunshanpeng.com/2019/02/24/使用同一个deployment实现金丝雀（灰度）发布/","excerpt":"","text":"一、发布前发布前，nginx运行三个实例（pod），副本（replicaset）版本编号855b564c8d，此时运行状态如下：123nginx-855b564c8d-5qhk7 1/1 Running 0 12h 172.20.199.135 10.22.19.14 &lt;none&gt;nginx-855b564c8d-hclp9 1/1 Running 0 12h 172.20.235.210 10.22.19.5 &lt;none&gt;nginx-855b564c8d-n7kn8 1/1 Running 0 12h 172.20.35.53 10.22.19.17 &lt;none&gt; 二、发布中步骤一：发布此时进行发布，新副本版本编号77b6c7b585，此时发布状态如下：1234nginx-77b6c7b585-vzc9g 0/1 Running 0 21s 172.20.42.156 10.22.19.15 &lt;none&gt;nginx-855b564c8d-5qhk7 1/1 Running 0 12h 172.20.199.135 10.22.19.14 &lt;none&gt;nginx-855b564c8d-hclp9 1/1 Running 0 12h 172.20.235.210 10.22.19.5 &lt;none&gt;nginx-855b564c8d-n7kn8 1/1 Running 0 12h 172.20.35.53 10.22.19.17 &lt;none&gt; 步骤二：暂停执行暂停命令kubectl rollout pause deployment -n qa nginx 等待第一个实例运行成功，此时状态如下： 1234nginx-77b6c7b585-vzc9g 1/1 Running 0 1m 172.20.42.156 10.22.19.15 &lt;none&gt;nginx-855b564c8d-5qhk7 1/1 Running 0 12h 172.20.199.135 10.22.19.14 &lt;none&gt;nginx-855b564c8d-hclp9 1/1 Running 0 12h 172.20.235.210 10.22.19.5 &lt;none&gt;nginx-855b564c8d-n7kn8 1/1 Running 0 12h 172.20.35.53 10.22.19.17 &lt;none&gt; 此时为暂停发布状态，三个旧版本实例正常运行，一个新版本实例运行成功，流量将分摊到这四个实例上。 步骤三：恢复执行恢复命令kubectl rollout resume deployment -n qa nginx 此时状态如下：1234nginx-77b6c7b585-4bt6h 0/1 Running 0 33s 172.20.190.56 10.22.19.6 &lt;none&gt;nginx-77b6c7b585-g4wnx 1/1 Running 0 1m 172.20.42.172 10.22.19.15 &lt;none&gt;nginx-855b564c8d-5qhk7 1/1 Running 0 12h 172.20.199.135 10.22.19.14 &lt;none&gt;nginx-855b564c8d-n7kn8 1/1 Running 0 12h 172.20.35.53 10.22.19.17 &lt;none&gt; 恢复后会执行正常的滚动升级操作，即新增一个新副本实例，删除一个旧副本实例。滚动升级时，新旧版本同时存在，总实例最大数量不会超过目标数量的25%，最小数量不会低于目标数量的75%。 三、发布完成实例的副本版本编号都变成77b6c7b585，说明全部升级成功，此时状态如下：123nginx-77b6c7b585-4bt6h 1/1 Running 0 36m 172.20.190.56 10.22.19.6 &lt;none&gt;nginx-77b6c7b585-g4wnx 1/1 Running 0 36m 172.20.42.172 10.22.19.15 &lt;none&gt;nginx-77b6c7b585-kcsqm 1/1 Running 0 2m 172.20.235.253 10.22.19.5 &lt;none&gt; 四、回滚发布中发现功能与预期不符，需要取消发布并回滚至上个发布版本，此时可以执行回滚操作。执行回滚命令kubectl rollout undo deployment -n qa nginx。回滚操作在发布中，发布后都可以执行。 发布中回滚当暂停发布进行灰度验证时，发现功能不符合预期，需要取消发布。暂停时灰度验证状态：1234nginx-77b6c7b585-vzc9g 1/1 Running 0 2m 172.20.42.156 10.22.19.15 &lt;none&gt;nginx-855b564c8d-5qhk7 1/1 Running 0 12h 172.20.199.135 10.22.19.14 &lt;none&gt;nginx-855b564c8d-hclp9 1/1 Running 0 12h 172.20.235.210 10.22.19.5 &lt;none&gt;nginx-855b564c8d-n7kn8 1/1 Running 0 12h 172.20.35.53 10.22.19.17 &lt;none&gt; 回滚后状态:123nginx-855b564c8d-5qhk7 1/1 Running 0 12h 172.20.199.135 10.22.19.14 &lt;none&gt;nginx-855b564c8d-jqbjk 1/1 Running 0 2m 172.20.235.215 10.22.19.5 &lt;none&gt;nginx-855b564c8d-n7kn8 1/1 Running 0 12h 172.20.35.53 10.22.19.17 &lt;none&gt; 副本编号变成855b564c8d，说明回滚成功，恢复到新版本。 发布完成后回滚全部升级成功后，发现功能异常，回滚到之前版本。回滚前状态：123nginx-77b6c7b585-4bt6h 1/1 Running 0 36m 172.20.190.56 10.22.19.6 &lt;none&gt;nginx-77b6c7b585-g4wnx 1/1 Running 0 36m 172.20.42.172 10.22.19.15 &lt;none&gt;nginx-77b6c7b585-kcsqm 1/1 Running 0 2m 172.20.235.253 10.22.19.5 &lt;none&gt; 回滚后状态：123nginx-855b564c8d-5qhk7 1/1 Running 0 6m 172.20.199.135 10.22.19.14 &lt;none&gt;nginx-855b564c8d-jqbjk 1/1 Running 0 6m 172.20.235.215 10.22.19.5 &lt;none&gt;nginx-855b564c8d-n7kn8 1/1 Running 0 6m 172.20.35.53 10.22.19.17 &lt;none&gt; 滚动升级状态目标实例数3个，旧实例数3个，总实例数3个：123nginx-855b564c8d-5qhk7 1/1 Running 0 12h 172.20.199.135 10.22.19.14 &lt;none&gt;nginx-855b564c8d-hclp9 1/1 Running 0 12h 172.20.235.210 10.22.19.5 &lt;none&gt;nginx-855b564c8d-n7kn8 1/1 Running 0 12h 172.20.35.53 10.22.19.17 &lt;none&gt; 目标实例数3个，旧实例数3个，新实例数1个，总实例数4个：1234nginx-77b6c7b585-vzc9g 1/1 Running 0 1m 172.20.42.156 10.22.19.15 &lt;none&gt;nginx-855b564c8d-5qhk7 1/1 Running 0 12h 172.20.199.135 10.22.19.14 &lt;none&gt;nginx-855b564c8d-hclp9 1/1 Running 0 12h 172.20.235.210 10.22.19.5 &lt;none&gt;nginx-855b564c8d-n7kn8 1/1 Running 0 12h 172.20.35.53 10.22.19.17 &lt;none&gt; 目标实例数3个，旧实例数2个，新实例数2个，总实例数4个：1234nginx-77b6c7b585-pmw2f 1/1 Running 0 15s 172.20.190.51 10.22.19.6 &lt;none&gt;nginx-77b6c7b585-vzc9g 1/1 Running 0 3m 172.20.42.156 10.22.19.15 &lt;none&gt;nginx-855b564c8d-5qhk7 1/1 Running 0 12h 172.20.199.135 10.22.19.14 &lt;none&gt;nginx-855b564c8d-n7kn8 1/1 Running 0 12h 172.20.35.53 10.22.19.17 &lt;none&gt; 目标实例数3个，旧实例数1个，新实例数3个，总实例数4个1234nginx-77b6c7b585-4bt6h 1/1 Running 0 35m 172.20.190.56 10.22.19.6 &lt;none&gt;nginx-77b6c7b585-g4wnx 1/1 Running 0 35m 172.20.42.172 10.22.19.15 &lt;none&gt;nginx-77b6c7b585-kcsqm 1/1 Running 0 1m 172.20.235.253 10.22.19.5 &lt;none&gt;nginx-855b564c8d-5qhk7 1/1 Running 0 13h 172.20.199.135 10.22.19.14 &lt;none&gt; 目标实例数3个，新实例数3个，总实例数3个123nginx-77b6c7b585-4bt6h 1/1 Running 0 36m 172.20.190.56 10.22.19.6 &lt;none&gt;nginx-77b6c7b585-g4wnx 1/1 Running 0 36m 172.20.42.172 10.22.19.15 &lt;none&gt;nginx-77b6c7b585-kcsqm 1/1 Running 0 2m 172.20.235.253 10.22.19.5 &lt;none&gt;","categories":[{"name":"容器","slug":"容器","permalink":"http://sunshanpeng.com/categories/容器/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"http://sunshanpeng.com/tags/k8s/"}]},{"title":"常用的IntelliJ IDEA 插件推荐","slug":"常用的IntelliJ IDEA 插件推荐","date":"2019-02-24T14:29:37.000Z","updated":"2019-08-18T04:22:51.991Z","comments":true,"path":"2019/02/24/常用的IntelliJ IDEA 插件推荐/","link":"","permalink":"http://sunshanpeng.com/2019/02/24/常用的IntelliJ IDEA 插件推荐/","excerpt":"","text":"一、Free Mybatis plugin https://plugins.jetbrains.com/plugin/8321-free-mybatis-plugin/ 1.把mybatis的xml文件和代码关联上，可以在mapper接口直接跳转到对应的SQL语句。 2.如果没有对应的SQL语句，可以快捷补全（当然具体SQL语句还是要自己写）。 二、Maven Helper https://plugins.jetbrains.com/plugin/7179-maven-helper/ 简单、方便的查看maven依赖和冲突 三、GenerateAllSetter https://plugins.jetbrains.com/plugin/9360-generateallsetter/ 一键调用一个对象的所有的set方法 四、RestfulToolkit https://plugins.jetbrains.com/plugin/10292-restfultoolkit/ 通过URL查询对应的方法，可以根据请求方法过滤，另外支持简单的HTTP调用。 五、Key Promoter X https://plugins.jetbrains.com/plugin/9792-key-promoter-x/ 帮助我们快速的掌握 IntelliJ IDEA的快捷键。 六、GsonFormat https://plugins.jetbrains.com/plugin/7654-gsonformat/ 快速方便的把json字符串转换为实体类（快捷键ALT+S）。 七、String Manipulation https://plugins.jetbrains.com/plugin/2162-string-manipulation/ 快捷方便的把字符串在驼峰、大小写等格式之间转换（快捷键Alt+M)。 八、Alibaba Java Coding Guidelines https://plugins.jetbrains.com/plugin/10046-alibaba-java-coding-guidelines/ 扫描代码是否符合规范 九、Lombok简化java bean的代码量，自动生成get set toString EqualsAndHashCode 构造器。 需要引入依赖包 12345&gt; &lt;dependency&gt; &gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &gt; &lt;/dependency&gt;&gt; 十、.ignore https://plugins.jetbrains.com/plugin/7495--ignore/ 使用协作或者打包工具时，方便的处理要忽略的文件和文件夹。","categories":[{"name":"java","slug":"java","permalink":"http://sunshanpeng.com/categories/java/"}],"tags":[{"name":"idea","slug":"idea","permalink":"http://sunshanpeng.com/tags/idea/"}]},{"title":"Kubectl常用命令","slug":"Kubectl常用命令","date":"2019-02-24T14:26:37.000Z","updated":"2019-08-16T18:19:38.363Z","comments":true,"path":"2019/02/24/Kubectl常用命令/","link":"","permalink":"http://sunshanpeng.com/2019/02/24/Kubectl常用命令/","excerpt":"","text":"运行容器前台运行的容器1kubectl run -it --rm --image=centos --restart=Never test bash 常驻后台的容器1kubectl run nginx --image=nginx --replicas=2 通常使用yaml文件创建容器，只在调试或者排错的时候使用kubectl run临时创建容器。 比如： 创建网络排查容器1kubectl run -it --rm --restart=Never --image=infoblox/dnstools:latest dnstools 如果创建了一个MySQL容器并且暴露了service，但是不能使用service访问mysql，这个时候可以使用nslookup或者dig这些网络命令排查。 123456dnstools# nslookup mysqlServer: 172.21.0.2Address: 172.21.0.2#53Name: mysql.qa.svc.cluster.localAddress: 172.21.147.86 创建MySQL排查容器1kubectl run -it --rm --image=mysql:5.6 --restart=Never mysql-client -- mysql -h mysql --port 3306 -u root -p123456 如果创建的MySQL容器在集群外连接不上了，可以创建一个mysql容器在内部连接看看能不能连上。 -h参数是MySQL的host； --port参数是MySQL的端口号； -u参数是MySQL的用户名； -p参数是用户名对应的密码，-p和密码之间没有空格。 应用资源1kubectl apply -f deployment.yaml 推荐使用这种方式创建或者更新资源。 获取容器1kubectl get pods --all-namespaces -o wide 获取所有namespace的容器。 --all-namespaces表示所有namespace，默认获取default namespace的资源，可以通过-n指定namespace。 -o wide表示输出格式，其他还要json，yaml。 获取所有不是Running状态的容器1kubectl get pods --all-namespaces -o wide | awk '&#123;if ($4 != \"Running\") print $0&#125;' 获取其他资源除了容器，还有很多其他资源例如node、service、deployment、statefulset、daemonset、job、pvc、pv等等可以通过kubectl来获取，大多数是区分namespace的，也有不区分namespace的比如node、pv、storageclass等。 删除容器1kubectl delete pod nginx-deployment-599c95f496-hd2jc 强制删除：1kubectl delete pod nginx-deployment-599c95f496-hd2jc --force --grace-period=0 批量强制删除：1kubectl get pods | grep Terminating | awk '&#123;print $1&#125;' | xargs kubectl delete pod --force --grace-period=0 批量强制删除非运行容器1kubectl get pods --all-namespaces | awk '&#123;if ($4 != \"Running\") system (\"kubectl -n \" $1 \" delete pods \" $2 \" --grace-period=0 \" \" --force \")&#125;' 扩缩容1kubectl scale deployment nginx --replicas 4 最小可以缩容到0个。 暂停滚动升级1kubectl rollout pause deployment nginx 滚动升级时，可以使用该命令暂停升级来实现金丝雀发布。 恢复滚动升级1kubectl rollout resume deployment nginx 暂停后恢复。 回滚1kubectl rollout undo deployment nginx 回滚到上一次发布。 给node加taint（污点）1kubectl taint nodes node1 key=value:NoSchedule 给节点添加污点后只有容忍了该污点的容器才能调度上来。 查看node信息1kubectl describe nodes node1 通过该命令可以查看node的资源、内核、容器、标签和污点等等。 给node加标签1kubectl label node node1 kubernetes.io/role=node --overwrite 给node加标签后可以用节点亲和性指定某些pod调度到固定node。 删除node上的标签1kubectl label node node1 kubernetes.io/role- 根据标签筛选1kubectl get nodes -l node-type=iot 禁止节点调度只禁止不驱逐1kubectl cordon node1 这种方式只把node标记为SchedulingDisabled,已经在node上运行的pod不会受影响，之后不会再有新的pod调度上去。 禁止并驱逐1kubectl drain node1 --ignore-daemonsets --delete-local-data --force 这种方式除了把node标记为SchedulingDisabled，已经运行的pod也会被驱逐，保证节点除daemonset外没有其他pod。 恢复调度1kubectl uncordon node1","categories":[{"name":"容器","slug":"容器","permalink":"http://sunshanpeng.com/categories/容器/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"http://sunshanpeng.com/tags/k8s/"}]},{"title":"Docker的本质","slug":"Docker的本质","date":"2019-02-24T14:26:37.000Z","updated":"2019-08-16T18:16:25.991Z","comments":true,"path":"2019/02/24/Docker的本质/","link":"","permalink":"http://sunshanpeng.com/2019/02/24/Docker的本质/","excerpt":"","text":"Docker容器其实是一种沙盒技术，能够像一个集装箱一样，把应用“装”起来。 这样应用与应用之间，就因为有了边界而不会相互干扰； 被装进集装箱的应用，也可以被方便地搬来搬去。 容器的本质，是由Namespace、Cgroups和rootfs三种技术构建出来的进程的隔离环境。 Docke容器技术的核心功能，是通过约束和修改进程的动态表现，从而为其创造出一个“边界”。 Namespace技术是用来修改进程视图的主要方法，Cgroups技术是用来制造约束的主要手段，rootfs技术是用来限制进程文件系统的主要方式。 Namespace：正常情况下，每当我们在宿主机上运行一个程序的时候，操作系统都会给它分配一个进程编号，比如PID=100。这是进程的唯一标识，就像员工的工号一样。 而通过Docker把程序运行在容器中的时候，利用Namespace技术给进程施一个“障眼法”，让它看不到其他的进程，误以为只有它一个进程。 这种机制对被隔离的应用的进程空间做了限制，使其只能看到重新计算过的进程编号，比如PID=1，但事实上它在宿主机的操作系统里，还是原来的100号进程。 除了PID Namespace，Linux系统还提供了Mount、UTS、IPC、Network和User这些Namespace，用来对各种不同的进程上下文进行“障眼法”操作。 比如用Mount Namespace让被隔离的进程只能看到当前Namespace里的挂载点信息；Network Namespace让被隔离的进程只能看到当前Namespace的网络设备和配置。 所以，实际上Docker容器只是指定了这个进程所需要启用的一组Namespace参数，从而让进程只能看到当前Namespace所限定的资源、文件、设备、状态和配置等，对宿主机和其他不相关的程序完全看不到。 所以也可以说，容器其实就是一种特殊的进程而已。 虚拟机和Docker容器比较 在理解Docker的时候，通常会用这张图来比较虚拟机和Docker。 左边画出了虚拟机的工作原理：其中Hypervisor是虚拟机最主要的部分，它通过硬件虚拟化功能，模拟出运行一个操作系统需要的各种硬件，比如CPU、内存、I/O设备等等。然后它在这些虚拟的硬件上安装了一个新的操作系统，即Guest OS。 这样，用户的应用进程就可以运行在这个虚拟机的机器中，它能够看到的自然也只有Guest OS的文件和目录，以及这个机器里的虚拟设备。 右边用一个名为Docker Engine的软件替换了Hypervisor，把虚拟机的概念套在了容器上，实际上这个说法是不严谨的，跟真正存在的虚拟机不同，实际上并没有Docker Engine这一层。 Docker帮助用户启动的还是原来的应用进程，只不过在创建这些进程时给它们加上了各种各样的Namespace参数。 这样，这些进程就会觉得自己是各自PID Namespace里的第1号进程，只能看到各自Mount Namespace里挂载的目录和文件，只能访问各自Network Namespace里的网络设备，就好像运行在一个个独立的容器里。 Docker的架构其实这样画更合适 如果用虚拟化技术作为应用沙盒，就必须要由Hypervisor来负责创建虚拟机，这个虚拟机是真实存在的，并且它里面必须运行一个完整的Guest OS才能执行用户的应用进程。这就不可避免的带来的额外的资源消耗和占用。 根据实验，一个运行着CentOS的KVM虚拟机启动后，在不做优化的情况下，虚拟机自己就需要占用100~·200内存。此外，用户应用运行在虚拟机里面，对宿主机操作系统的调用就不可避免地进过虚拟化软件的拦截和处理，这本身又是一层性能损失。 使用Docker容器化后的用户应用，依然还是一个宿主机上的普通进程，这就意味着因为虚拟化而带来的性能损耗都是不存在的； 另一方面，使用Namespace作为隔离手段的容器并不需要单独的Guest OS，这使得容器额外的资源占用几乎可以忽略不计。 所以“敏捷”和“高性能”是容器相较于虚拟化最大的优势。 当然有利就有弊，Docker容器相较于虚拟化最大的弊端就是隔离的不彻底。 因为容器只是运行在宿主机上的一种特殊的进程，所以多个容器之间使用的还是同一个宿主机的操作系统内核。 另外，很多资源和对象不能被Namespace化，比如：时间。 Cgroups：虽然容器内的进程在Namespace的作用下只能看到容器里的情况，但是在宿主机上，它作为第100号进程，与其它所有的进程之间依然是平等的竞争关系。 这就意味着，虽然这个进程表面上被隔离起来了，但它所能够使用到的资源（CPU、内存），却是可以随时被宿主机的其他进程（或者其他容器）占用。 当然这个进程也可能把所有的资源吃光，而Linux Cgroups就是Linux内核中用来为进程设置资源限制的一个重要功能。 跟Namespace的情况类似，Cgroups对资源的限制能力也有很多不完善的地方，比如/proc文件系统的问题。 /proc目录存储的是记录当前内核运行状态的一系列特殊文件，用户可以通过访问这些文件，查看系统以及当前正在运行的进程的信息，比如CPU使用情况，内存占用率等。 当我们在容器内执行top命令的时候，会发现它显示的信息是宿主机的CPU和内存，而不是当前容器的数据。 rootfs：在Linux操作系统里，有一个名为chroot的命令，作用是帮助我们“change root file”，即改变进程的根目录到指定的位置。 而对于被chroot的进程来说，它并不知道自己的根目录已经被修改了。Mount Namespace就是基于chroot的不断改良被发明出来的。 为了让容器的根目录更加真实，一般会在这个容器的根目录下挂载一个完整操作系统的文件系统，比如Ubuntu 16.04的ISO。 这样，在容器启动之后，我们在容器里通过执行“ls /”查看根目录下的内容，就是Ubuntu 16.04的所有目录和文件。 这个挂载在容器根目录上，用来为容器进程提供隔离后执行环境的文件系统，就是容器镜像，专业名字rootfs(根文件系统)。 一个常见的rootfs，或者说容器镜像，会包含比如bin boot dev etc home lib lib64 media mnt opt proc root run sbin srv sys test tmp usr var等的目录和文件。 进入容器后执行的/bin/bash，就是/bin目录下的可执行文件，与宿主机的/bin/bash完全不同。 rootfs只是一个操作系统所包含的文件、配置和目录，并不包括操作系统内核。 所以说rootfs只包括了操作系统的“躯壳”，并没有包括操作系统的“灵魂”，同一台机器上的所有容器，都是共享宿主机操作系统的内核。 这意味着，容器进程需要配置内核参数、加载额外的内核模块，以及跟内核进行直接交互的时候，需要注意这些操作和依赖的对象，都是宿主机操作系统的内核，对该机器上的所有容器生效。 不过也真是由于rootfs，容器才有了一个被反复宣传至今的重要特性：一致性。 由于rootfs里打包的不只是应用，而是整个操作系统的文件和目录，也就意味着，应用以及它运行所需要的所有依赖，都被封装在了一起。 有了容器镜像“打包操作系统”的能力，这个最基础的依赖环境也变成了应用沙盒的一部分，这就赋予了容器的所谓一致性。 无论在本地、云端，还是在任何机器上，用户只需要解压打包好的容器镜像，那么这个应用运行所需要的完整的执行环境就被重现出来了。 这种深入到操作系统级别的运行环境一致性，打通了应用在本地开发和远端执行环境之间难以逾越的鸿沟。 另外，Docker在镜像的设计中，引入了层（layer）的概念，也就是说，用户制作镜像的每一步操作，都会生成一个层，也就是一个增量的rootfs。 这个设计用到了一种叫做联合文件系统（Union File System）的能力，也叫UnionFS,最主要的功能是将多个不同位置的目录联合挂载到同一个目录下。 如图所示，容器中的rootfs分为只读层、Init层、可读写层。 只读层：包含了操作系统的一部分。 Init层：夹在只读层和可读写层中间，是Docker项目单独生成的一个内部层，用来存在/etc/hosts、/etc/resolv.conf等信息。 可读写层：用来存放修改rootfs后产生的增量，无论增、删、改，都发生在这里。 使用docker commit和push指令保存并上传被修改过的容器的时候，只读层和Init层不会有任何变化，变化的只是可读写层。 Dockerfile实际操作中，我们不会使用docker commit命令来把一个运行中的docker容器提交成为一个docker镜像。 使用 docker commit 意味着所有对镜像的操作都是黑箱操作，生成的镜像也被称为黑箱镜像，换句话说，就是除了制作镜像的人知道执行过什么命令、怎么生成的镜像，别人根本无从得知。 另外结合之前的分层，任何修改的结果仅仅是在当前层进行标记、添加、修改，而不会改动上一层。 这样每一次修改都会让镜像更加臃肿一次，所删除的上一层的东西并不会丢失，会一直如影随形的跟着这个镜像，即使根本无法访问到。 把每一层修改、安装、构建、操作的命令都写入一个脚本，用这个脚本来构建、定制镜像，那么无法重复的问题、镜像构建透明性的问题、体积的问题就都会解决，这个脚本就是 Dockerfile。 Dockerfile 是一个文本文件，其内包含了一条条的指令(Instruction)，每一条指令构建一层，因此每一条指令的内容，就是描述该层应当如何构建。 每一个 RUN 的行为，就和手动docker commit建立镜像的过程一样：新建立一层，在其上执行这些命令，执行结束后，commit 这一层的修改，构成新的镜像。 Union FS 是有最大层数限制的，比如 AUFS，曾经是最大不得超过 42 层，现在是不得超过 127 层。","categories":[{"name":"容器","slug":"容器","permalink":"http://sunshanpeng.com/categories/容器/"}],"tags":[{"name":"docker","slug":"docker","permalink":"http://sunshanpeng.com/tags/docker/"}]},{"title":"CentOS升级内核kernel的几种方式","slug":"CentOS升级内核kernel的几种方式","date":"2019-01-17T12:18:01.000Z","updated":"2019-08-18T14:17:48.357Z","comments":true,"path":"2019/01/17/CentOS升级内核kernel的几种方式/","link":"","permalink":"http://sunshanpeng.com/2019/01/17/CentOS升级内核kernel的几种方式/","excerpt":"","text":"小版本升级1234# installyum install kernel* -y# rebootinit 6 安装最新稳定版内核1234567891011121314# import keyrpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org# install elrepo reporpm -Uvh http://www.elrepo.org/elrepo-release-7.0-2.el7.elrepo.noarch.rpm# list kernelyum --disablerepo=\\* --enablerepo=elrepo-kernel list kernel*# install kernelyum --enablerepo=elrepo-kernel install kernel-ml-devel kernel-ml -y# yum --disablerepo=\\* --enablerepo=elrepo-kernel install -y kernel-ml.x86_64# modify grubgrub2-set-default 0grub2-mkconfig -o /boot/grub2/grub.cfg# rebootreboot 安装指定版本内核推荐一个可以找到各个版本内核的国内镜像站：http://mirror.rc.usf.edu/compute_lock/elrepo/kernel/el7/x86_64/RPMS 本次安装以4.19版本的内核示例： 1234567# installrpm -ivh http://mirror.rc.usf.edu/compute_lock/elrepo/kernel/el7/x86_64/RPMS/kernel-ml-4.19.12-1.el7.elrepo.x86_64.rpm# modify grubsed -i \"s/GRUB_DEFAULT=saved/GRUB_DEFAULT=0/\" /etc/default/grubgrub2-mkconfig -o /boot/grub2/grub.cfg# rebootreboot 编译源码安装没试过 参考https://mritd.me/2016/11/08/update-centos-kernel/","categories":[{"name":"linux","slug":"linux","permalink":"http://sunshanpeng.com/categories/linux/"}],"tags":[{"name":"kernel","slug":"kernel","permalink":"http://sunshanpeng.com/tags/kernel/"}]},{"title":"创建NFS的StorageClass","slug":"创建NFS的StorageClass","date":"2018-12-17T08:26:37.000Z","updated":"2019-08-18T13:54:01.327Z","comments":true,"path":"2018/12/17/创建NFS的StorageClass/","link":"","permalink":"http://sunshanpeng.com/2018/12/17/创建NFS的StorageClass/","excerpt":"","text":"前言在Kubernetes的几种网络存储中，NFS是成本较低、使用简单的一种方案。 但是NFS存储不建议用在生产环境，因为我们测试环境的MySQL数据库部署在NFS上都经常出问题，比如nfs4_reclaim_open_state: Lock reclaim failed和kernel:NMI watchdog: BUG: soft lockup - CPU#0 stuck for 26s。 NFS服务端12345678910111213141516171819yum install -y net-tools lsof nfs-utils rpcbindmkdir /data/nfs -pvi /etc/exports#挂载NFS服务器上的/data/nfs/目录到自己的文件系统中，rw表示可读写，no_root_squash 是让root保持权限/data/nfs/ *(insecure,rw,no_root_squash)关闭防火墙systemctl stop firewalld先为rpcbind和nfs做开机启动：(必须先启动rpcbind服务)systemctl enable rpcbind.servicesystemctl enable nfs-server.service然后分别启动rpcbind和nfs服务：systemctl start rpcbind.servicesystemctl start nfs-server.service exportfs -r#可以查看到已经okexportfs/home/nfs 192.168.248.0/24 NFS客户端123456789101112#安装nfs工具yum install -y nfs-utils#建立挂载目录mkdir /data#挂载nfsmount -t nfs 192.168.80.145:/data/nfs /data卸载挂载umount /data 查看是目录挂载状态df -hshowmount -e 192.168.80.145 创建NFS-StorageClassnfs-rbac.yaml123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657kind: ServiceAccountapiVersion: v1metadata: name: nfs-client-provisioner---kind: ClusterRoleapiVersion: rbac.authorization.k8s.io/v1metadata: name: nfs-client-provisioner-runnerrules: - apiGroups: [\"\"] resources: [\"persistentvolumes\"] verbs: [\"get\", \"list\", \"watch\", \"create\", \"delete\"] - apiGroups: [\"\"] resources: [\"persistentvolumeclaims\"] verbs: [\"get\", \"list\", \"watch\", \"update\"] - apiGroups: [\"storage.k8s.io\"] resources: [\"storageclasses\"] verbs: [\"get\", \"list\", \"watch\"] - apiGroups: [\"\"] resources: [\"events\"] verbs: [\"create\", \"update\", \"patch\"]---kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1metadata: name: run-nfs-client-provisionersubjects: - kind: ServiceAccount name: nfs-client-provisioner namespace: defaultroleRef: kind: ClusterRole name: nfs-client-provisioner-runner apiGroup: rbac.authorization.k8s.io---kind: RoleapiVersion: rbac.authorization.k8s.io/v1metadata: name: leader-locking-nfs-client-provisionerrules: - apiGroups: [\"\"] resources: [\"endpoints\"] verbs: [\"get\", \"list\", \"watch\", \"create\", \"update\", \"patch\"]---kind: RoleBindingapiVersion: rbac.authorization.k8s.io/v1metadata: name: leader-locking-nfs-client-provisionersubjects: - kind: ServiceAccount name: nfs-client-provisioner namespace: defaultroleRef: kind: Role name: leader-locking-nfs-client-provisioner apiGroup: rbac.authorization.k8s.io nfs-deployment.yaml1234567891011121314151617181920212223242526272829303132333435363738kind: DeploymentapiVersion: extensions/v1beta1metadata: name: nfs-client-provisionerspec: replicas: 1 strategy: type: Recreate template: metadata: labels: app: nfs-client-provisioner spec: tolerations: - key: node-env value: pre effect: NoSchedule operator: Equal priorityClassName: cluster serviceAccountName: nfs-client-provisioner containers: - name: nfs-client-provisioner image: quay.io/external_storage/nfs-client-provisioner:latest volumeMounts: - name: nfs-client-root mountPath: /persistentvolumes env: - name: PROVISIONER_NAME value: nfs-provisioner - name: NFS_SERVER value: ##NFS_SERVER_IP## - name: NFS_PATH value: ##NFS_PATH## volumes: - name: nfs-client-root nfs: server: ##NFS_SERVER_IP## path: ##NFS_PATH## nfs-storage.yaml1234567apiVersion: storage.k8s.io/v1kind: StorageClassmetadata: name: nfs-storageprovisioner: nfs-provisioner # or choose another name, must match deployment's env PROVISIONER_NAME'parameters: archiveOnDelete: \"false\" test-nfs-storage.yaml123456789101112131415161718192021222324252627282930313233343536kind: PersistentVolumeClaimapiVersion: v1metadata: name: test-claim annotations: volume.beta.kubernetes.io/storage-class: \"nfs-storage\"spec: accessModes: - ReadWriteMany resources: requests: storage: 1Mi---kind: PodapiVersion: v1metadata: name: test-podspec: containers: - name: test-pod image: busybox command: - \"/bin/sh\" args: - \"-c\" - \"touch /mnt/SUCCESS &amp;&amp; exit 0 || exit 1\" volumeMounts: - name: nfs-pvc mountPath: \"/mnt\" restartPolicy: \"Never\" volumes: - name: nfs-pvc persistentVolumeClaim: claimName: test-claim 创建顺序 nfs-rbac.yaml nfs-deployment.yaml nfs-storage.yaml 参数说明nfs-deployment.yaml ##NFS_SERVER_IP##是NFS服务端的IP，根据实际IP进行替换。 ##NFS_PATH##是NFS服务端的目录，根据实际目录进行替换。 部署12345NFS_SERVER_IP=192.168.80.145 #换成自己的实际IPNFS_PATH=/data/nfs # 换成自己的实际目录kubectl apply -f nfs-rbac.yamlsed \"s|##NFS_SERVER_IP##|$&#123;NFS_SERVER_IP&#125;|g;s|##NFS_PATH##|$&#123;NFS_PATH&#125;|g\" nfs-deployment.yaml | kubectl apply -f -kubectl apply -f nfs-storage.yaml 验证方法1kubectl apply -f test-nfs-storage.yaml 可选：设置默认存储设置这个StorageClass为Kubernetes的默认存储1234 kubectl patch storageclass nfs-storage -p '&#123;\"metadata\": &#123;\"annotations\":&#123;\"storageclass.kubernetes.io/is-default-class\":\"true\"&#125;&#125;&#125;' [root@master1 nfs-storage]# kubectl get scNAME PROVISIONER AGEnfs-storage (default) nfs-provisioner 12m 参考https://www.cnblogs.com/lixiuran/p/7117000.html https://github.com/kubernetes-incubator/external-storage/tree/master/nfs-client","categories":[{"name":"容器","slug":"容器","permalink":"http://sunshanpeng.com/categories/容器/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"http://sunshanpeng.com/tags/k8s/"}]},{"title":"下载谷歌镜像的几种姿势","slug":"下载谷歌镜像的几种姿势","date":"2018-11-28T08:26:37.000Z","updated":"2019-08-18T12:49:58.168Z","comments":true,"path":"2018/11/28/下载谷歌镜像的几种姿势/","link":"","permalink":"http://sunshanpeng.com/2018/11/28/下载谷歌镜像的几种姿势/","excerpt":"","text":"前言在国内，因为墙的存在所以很多国外网站不能访问，这其中就有谷歌镜像网站gcr.io，不过我们可以通过其他方式使用谷歌的镜像。 通过国内镜像站阿里云镜像站域名：registry.cn-hangzhou.aliyuncs.com/google_containers 微软镜像站域名：gcr.azk8s.cn/google_containers 中科大镜像站（拉取速度较慢）域名：gcr.mirrors.ustc.edu.cn/google_containers 使用方式替换谷歌镜像地址为国内镜像站地址，比如： k8s.gcr.io开头的k8s.gcr.io/coredns:1.1.3 12docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/coredns:1.1.3docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/coredns:1.1.3 k8s.gcr.io/coredns:1.1.3 gcr.io开头的gcr.io/google_containers/heapster-amd64:v1.5.3 12docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/heapster-amd64:v1.5.3docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/heapster-amd64:v1.5.3 gcr.io/google_containers/heapster-amd64:v1.5.3 通过配置代理前提是有一台能够科学上网的机器，并且目标机器能访问到可以科学上网的机器。 为docker服务创建systemd插件目录： 1mkdir -p /etc/systemd/system/docker.service.d 配置代理文件 1234cat &gt;/etc/systemd/system/docker.service.d/http-proxy.conf&lt;EOF [Service] Environment=\"HTTP_PROXY=http://proxy.example.com:80/\"EOF 刷新配置并重启Docker 12systemctl daemon-reloadsystemctl restart docker 严重配置是否加载 12systemctl show --property=Environment dockerEnvironment=HTTP_PROXY=http://proxy.example.com:80/ 如果配置已经加载但还是不能下载谷歌镜像，可以试试把HTTP_PROXY改成http_proxy，我的配置是Environment=&quot;http_proxy=http://10.208.204.147:1080/&quot;才能使用。 HTTPS_PROXY和NO_PROXY配置类似，具体可以看官网代理配置。 通过脚本1curl -sSL https://git.io/getgcr | bash -s k8s.gcr.io/kube-apiserver:v1.14.3 把k8s.gcr.io/kube-apiserver:v1.14.3替换成要下载的目标镜像即可。 该方法本质上还是通过国内镜像站下载的。 参考http://mirror.azure.cn/help/gcr-proxy-cache.html https://blog.docker.com/2015/10/registry-proxy-cache-docker-open-source/ https://stackoverflow.com/questions/23111631/cannot-download-docker-images-behind-a-proxy","categories":[{"name":"容器","slug":"容器","permalink":"http://sunshanpeng.com/categories/容器/"}],"tags":[{"name":"docker","slug":"docker","permalink":"http://sunshanpeng.com/tags/docker/"}]},{"title":"使用ephemeral-storage管理容器的临时存储","slug":"使用ephemeral-storage管理容器的临时存储","date":"2018-11-28T08:26:37.000Z","updated":"2019-08-18T11:55:24.670Z","comments":true,"path":"2018/11/28/使用ephemeral-storage管理容器的临时存储/","link":"","permalink":"http://sunshanpeng.com/2018/11/28/使用ephemeral-storage管理容器的临时存储/","excerpt":"","text":"ephemeral-storage介绍Kubernetes在1.8的版本中引入了一种类似于CPU，内存的新的资源模式：ephemeral-storage，并且在1.10的版本在kubelet中默认打开了这个特性。 Alpha release target (x.y): 1.7/1.8 Beta release target (x.y): 1.10 Stable release target (x.y): 1.11 ephemeral-storage是为了管理和调度Kubernetes中运行的应用的短暂存储。 在每个Kubernetes的节点上，kubelet的根目录(默认是/var/lib/kubelet)和日志目录(/var/log)保存在节点的主分区上，这个分区同时也会被Pod的EmptyDir类型的volume、容器日志、镜像的层、容器的可写层所占用。ephemeral-storage便是对这块主分区进行管理，通过应用定义的需求(requests)和约束(limits)来调度和管理节点上的应用对主分区的消耗。 ephemeral-storage的eviction逻辑在节点上的kubelet启动的时候，kubelet会统计当前节点的主分区的可分配的磁盘资源，或者你可以覆盖节点上kubelet的配置来自定义可分配的资源。在创建Pod时会根据存储需求调度到满足存储的节点，在Pod使用超过限制的存储时会对其做驱逐的处理来保证不会耗尽节点上的磁盘空间。 如果运行时指定了别的独立的分区，比如修改了docker的镜像层和容器可写层的存储位置(默认是/var/lib/docker)所在的分区，将不再将其计入ephemeral-storage的消耗。 EmptyDir 的使用量超过了他的 SizeLimit，那么这个 pod 将会被驱逐 Container 的使用量（log，如果没有 overlay 分区，则包括 imagefs）超过了他的 limit，则这个 pod 会被驱逐 Pod 对本地临时存储总的使用量（所有 emptydir 和 container）超过了 pod 中所有container 的 limit 之和，则 pod 被驱逐 ephemeral-storage使用和内存和CPU的限制类似，存储的限制也是定义在Pod的container中 spec.containers[].resources.limits.ephemeral-storage spec.containers[].resources.requests.ephemeral-storage 示例： 12345678910111213141516apiVersion: v1kind: Podmetadata: name: teststorage labels: app: teststoragespec: containers: - name: teststorage image: nginx:1.14 command: [\"bash\", \"-c\", \"while true; do dd if=/dev/zero of=$(date '+%s').out count=1 bs=10MB; sleep 1; done\"] # 持续写入文件到容器的rootfs中 resources: limits: ephemeral-storage: 100Mi #定义存储的限制为100M requests: ephemeral-storage: 100Mi 12345[root@master1 ~]# kubectl get pods -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODEteststorage 1/1 Running 0 7s 172.20.189.69 10.208.204.35 &lt;none&gt;-------------------------------------------------------------------------------------------------teststorage 0/1 Evicted 0 1m &lt;none&gt; 10.208.204.35 &lt;none&gt; 1234567891011121314151617181920212223242526272829303132333435363738394041424344[root@master1 ~]# kubectl describe pod teststorage Name: teststorageNamespace: defaultNode: 10.208.204.35/Start Time: Wed, 28 Nov 2018 13:48:37 +0800Labels: app=teststorageAnnotations: kubectl.kubernetes.io/last-applied-configuration=&#123;\"apiVersion\":\"v1\",\"kind\":\"Pod\",\"metadata\":&#123;\"annotations\":&#123;&#125;,\"labels\":&#123;\"app\":\"teststorage\"&#125;,\"name\":\"teststorage\",\"namespace\":\"default\"&#125;,\"spec\":&#123;\"contai...Status: FailedReason: EvictedMessage: Pod ephemeral local storage usage exceeds the total limit of containers 100Mi. IP: Containers: teststorage: Image: nginx:1.14 Port: &lt;none&gt; Host Port: &lt;none&gt; Command: bash -c while true; do dd if=/dev/zero of=$(date '+%s').out count=1 bs=10MB; sleep 1; done Limits: ephemeral-storage: 100Mi Requests: ephemeral-storage: 100Mi Environment: &lt;none&gt; Mounts: /var/run/secrets/kubernetes.io/serviceaccount from default-token-mqzrh (ro)Volumes: default-token-mqzrh: Type: Secret (a volume populated by a Secret) SecretName: default-token-mqzrh Optional: falseQoS Class: BestEffortNode-Selectors: &lt;none&gt;Tolerations: &lt;none&gt;Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 1m default-scheduler Successfully assigned default/teststorage to 10.208.204.35 Normal Pulled 1m kubelet, 10.208.204.35 Container image \"nginx:1.14\" already present on machine Normal Created 1m kubelet, 10.208.204.35 Created container Normal Started 1m kubelet, 10.208.204.35 Started container Warning Evicted 8s kubelet, 10.208.204.35 Pod ephemeral local storage usage exceeds the total limit of containers 100Mi. Normal Killing 8s kubelet, 10.208.204.35 Killing container with id docker://teststorage:Need to kill Pod 参考文档：https://v1-11.docs.kubernetes.io/docs/tasks/configure-pod-container/assign-cpu-resource/ https://github.com/kubernetes/enhancements/issues/361 https://yq.aliyun.com/articles/594066 http://www.k8smeetup.com/article/VyEncpgA7","categories":[{"name":"容器","slug":"容器","permalink":"http://sunshanpeng.com/categories/容器/"}],"tags":[{"name":"k8s","slug":"k8s","permalink":"http://sunshanpeng.com/tags/k8s/"}]}]}